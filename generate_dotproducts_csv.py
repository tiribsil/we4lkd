##################################################
## Generates the csv files containing the dot product between the embeddings for all compounds in the corpus and the target disease's.
##################################################

# IMPORTS:
import os, torch, sys, shutil
import re

from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from gensim import models
from gensim.models import Word2Vec, FastText
import numpy as np
from scipy.special import softmax
import pandas as pd
from transformers import AutoTokenizer, AutoModelForMaskedLM

from target_disease import target_disease, normalized_target_disease


def clear_hugging_face_cache_folder(dirpath='/home/ac4mvvb/.cache/huggingface/hub/'):
    """ Clears the Hugging Face cache folder, to prevent memory error.

    Args:
        dirpath: the path of the folder.
    """

    for filename in os.listdir(dirpath):
        filepath = os.path.join(dirpath, filename)
        try:
            shutil.rmtree(filepath)
        except OSError:
            os.remove(filepath)


def get_token_embedding(word, embedding_matrix, method='mean'):
    """Access and returns the token embedding from the model's embedding matrix for a given word.
    Depending on the method selected, it can return different composed token embeddings.

    Args:
        word: token that the user wants to tokenize and access the token embedding;
        embedding_matrix: the model's embedding matrix, generated by model.get_input_embeddings();
        method: the selected way to compose the token embedding
            mean: compute the mean between all the subwords generated by the tokenization of the input;
            first: select the token embedding of the first subword generated by the tokenization of the input;
            last: select the token embedding of the first subword generated by the tokenization of the input.

    Returns:
        A torch tensor of 768 dimensions representing the token embedding.
    """
    tokenizer = AutoTokenizer.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext")

    vocab_ids = []
    tokenized_word = tokenizer.tokenize(word)

    for w in tokenized_word:
        vocab_ids.append(tokenizer.vocab[w])

    if method == 'mean':
        input_ids = torch.tensor(vocab_ids)
        word_token_embedding = embedding_matrix(input_ids)
        word_token_embedding = word_token_embedding.mean(0)

    elif method == 'first':
        input_ids = torch.tensor(vocab_ids[0])
        word_token_embedding = embedding_matrix(input_ids)

    elif method == 'last':
        input_ids = torch.tensor(vocab_ids[-1])
        word_token_embedding = embedding_matrix(input_ids)

    return word_token_embedding


def generate_compound_historical_record(compound):
    for method in ['first', 'last', 'mean']:
        bert_embeddings_files = sorted(
            [f.path for f in os.scandir(f'./{normalized_target_disease}/validation/bert/') if f.name.endswith('.pt') and method in f.name])

        compound_dict = {
            'year': [],
            'dot_product_result': [],
            'dot_product_result_absolute': [],
            'softmax': [],
            'normalized_dot_product_absolute': [],
            'standardized_dot_product_absolute': [],
            'softmax_normalization': [],
            'softmax_standardization': [],
        }

        for be in bert_embeddings_files:
            tensor_dict = torch.load(be)

            try:
                index_compound = tensor_dict['compound'].index(c)
                index_AML = tensor_dict['compound'].index('AML')

                AML_we = tensor_dict['word_embedding'][index_AML]
                compound_we = tensor_dict['word_embedding'][index_compound]

            except:
                continue

            dot_product = torch.dot(compound_we, AML_we).type(torch.DoubleTensor).item()
            if dot_product > 0:
                dot_product_absolute = dot_product

            else:
                dot_product_absolute = -1 * dot_product

            compound_dict['year'].append(int(be.split('.pt')[0][-4:]))
            compound_dict['dot_product_result'].append(dot_product)
            compound_dict['dot_product_result_absolute'].append(dot_product_absolute)
        print('Dot products computed')

        # Softmax:
        compound_dict['softmax'] = softmax(compound_dict['dot_product_result_absolute'])

        # Normalization:
        maximum = np.max(compound_dict['dot_product_result_absolute'])
        compound_dict['normalized_dot_product_absolute'] = [x / maximum for x in
                                                            compound_dict['dot_product_result_absolute']]
        compound_dict['softmax_normalization'] = softmax(compound_dict['normalized_dot_product_absolute'])

        # Standardization:
        # compound_dict['softmax_standardization'] = preprocessing.scale(compound_dict['dot_product_result_absolute'])
        mean = np.mean(compound_dict['dot_product_result_absolute'])
        standard_deviation = np.std(compound_dict['dot_product_result_absolute'])
        compound_dict['standardized_dot_product_absolute'] = [(x - mean) / standard_deviation for x in
                                                              compound_dict['dot_product_result_absolute']]
        compound_dict['softmax_standardization'] = softmax(compound_dict['standardized_dot_product_absolute'])

        print("Writing compound's historical record .csv file, method {}".format(method))
        pd.DataFrame.from_dict(data=compound_dict).to_csv(
            './validation/per_compound/bert/{}_{}.csv'.format(compound, method),
            columns=['year', 'dot_product_result', 'dot_product_result_absolute', 'softmax',
                     'normalized_dot_product_absolute', 'standardized_dot_product_absolute', 'softmax_normalization',
                     'softmax_standardization'], index=False)


def get_w2v_output_embedding(word, model, method):
    """ Returns the output embedding of a given word from Word2Vec or FastText model.
        Updated for Gensim 4.0.0+.

    Args:
        word: the token in model's vocabulary that you want the output embedding;
        model: Word2Vec or FastText model object;
        method: how to access the output embedding:
            'da': direct access to the token (i.e., model.wv[word]) if it exists in the vocabulary.
                  If not, it searches for the first token in the vocabulary that contains 'word' as a substring.
            'avg': compute the average of all words in vocabulary that contains 'word' as a substring.
    """
    if method == 'da':
        # Tenta o acesso direto, que é a forma mais limpa e rápida.
        if word in model.wv.key_to_index:
            return model.wv[word]

        # Se falhar, busca pela primeira palavra que contém 'word' como substring.
        # Iteramos sobre a lista de chaves (palavras) do vocabulário.
        for key in model.wv.index_to_key:
            if word in key:
                # Retorna o vetor da primeira correspondência encontrada.
                return model.wv[key]

        # Se nenhuma correspondência for encontrada, retorna None ou um vetor de zeros.
        # Isso evita que o programa quebre se a palavra não for encontrada de forma alguma.
        # A escolha depende de como o código que chama esta função lida com falhas.
        # Retornar None é mais explícito.
        # Se precisar de um vetor, use np.zeros(model.vector_size)
        return None

    elif method == 'avg':
        # Encontra todas as palavras no vocabulário que contêm 'word' como substring.
        tokens_containing_the_word = [key for key in model.wv.index_to_key if word in key]

        if not tokens_containing_the_word:
            # Se a lista estiver vazia, nenhuma palavra contém a substring.
            # Retorna None ou um vetor de zeros.
            return None

        # Coleta os vetores de todas as palavras correspondentes.
        # O acesso model.wv[tokens_containing_the_word] já retorna uma matriz numpy dos vetores.
        # É muito mais eficiente do que um loop.
        output_embeddings = model.wv[tokens_containing_the_word]

        # Calcula a média ao longo do eixo 0 (a média dos vetores).
        return np.mean(output_embeddings, axis=0)

    else:
        # Lida com um métod inválido, se necessário.
        raise ValueError(f"Método '{method}' inválido. Escolha 'da' ou 'avg'.")


def get_compounds():
    all_chemical_from_ner = pd.read_csv(f'./data/{normalized_target_disease}/ner_table.csv')
    return all_chemical_from_ner['token'].tolist()

# MAIN PROGRAM:
if __name__ == '__main__':
    print('Starting')

    all_compounds_from_ner = get_compounds()

    VALIDATION_TYPE = 'ft'  # must be either 'w2v' or 'ft'
    if VALIDATION_TYPE not in ['w2v', 'ft']:
        print('Invalid validation type, has to be either "w2v" or "ft".')
        exit(1)
    combination = '15' if VALIDATION_TYPE == 'w2v' else '16'

    model_directory_path = f'./data/{normalized_target_disease}/{VALIDATION_TYPE}/models_yoy_combination{combination}/'
    validation_directory_path = f'./data/{normalized_target_disease}/validation/per_compound/{VALIDATION_TYPE}/'

    os.makedirs(model_directory_path, exist_ok=True)
    os.makedirs(validation_directory_path, exist_ok=True)

    models = sorted([f.path for f in os.scandir(model_directory_path) if f.name.endswith('.model')])
    dictionary_for_all_compounds = {}

    for c in all_compounds_from_ner:
        dictionary_for_all_compounds.update({
            f'{c}_comb{combination}': {
                'year': [],
                'dot_product_result': [],
                'dot_product_result_absolute': [],
                'softmax': [],
                'normalized_dot_product_absolute': [],
                'standardized_dot_product_absolute': [],
                'softmax_normalization': [],
                'softmax_standardization': [],
            }
        }
        )

    # list of years which models were trained (the length of this list is the number of models trained):
    years = []

    for file in os.listdir(f'./data/{normalized_target_disease}/aggregated_results/'):
        file_path = f'./data/{normalized_target_disease}/aggregated_results/{file}'

        filename, extension = os.path.splitext(file_path)
        years.append(int(filename[-4:]))

    years = sorted(years)

    ########################
    # DEBUGGING:
    print('VALIDATION_TYPE: ', VALIDATION_TYPE)
    print('Years: {} to {}'.format(years[0], years[-1]))
    ########################

    # Step 1: access the token embedding of each compound after it appears for the first time in the corpus and save it into a .pt file
    for y in years:
        print(f'\nCurrent year of analysis: {y}')

        if VALIDATION_TYPE == 'w2v':
            # loading Word2Vec model from file:
            print('Loading Word2Vec model')
            model_comb15 = Word2Vec.load([x for x in models if str(y) in x][0])

            # accessing the word embedding of AML:
            try:
                target_disease_we = model_comb15.wv[normalized_target_disease]
                print(f"Accessing the word embedding of '{normalized_target_disease}'")

            except:
                print('Target disease is not in the vocabulary')
                continue

            # accessing the output embedding of each of the compounds present in the papers published until year y:
            for compound in all_compounds_from_ner:
                # METHOD 1: Direct Access to the token in the model's vocab
                print('Accessing the output embedding of {}'.format(compound))
                compound_we_comb15 = get_w2v_output_embedding(compound, model_comb15, method='da')
                if compound_we_comb15 is None:
                    print(f"Compound '{compound}' not found in the model's vocabulary.")
                    continue

                dot_product_comb15 = np.dot(compound_we_comb15, target_disease_we).item()

                dictionary_for_all_compounds['{}_comb15'.format(compound)]['year'].append(y)
                dictionary_for_all_compounds['{}_comb15'.format(compound)]['dot_product_result'].append(
                    dot_product_comb15)
                dictionary_for_all_compounds['{}_comb15'.format(compound)][
                    'dot_product_result_absolute'].append(abs(dot_product_comb15))

        elif VALIDATION_TYPE == 'ft':
            # loading FastText model from file:
            print('Loading FastText model')
            model_comb16 = FastText.load([x for x in models if str(y) in x][0])

            # accessing the word embedding of AML:
            try:
                target_disease_we = model_comb16.wv[normalized_target_disease]
                print(f"Accessing the word embedding of '{normalized_target_disease}'")

            except:
                print('Target disease is not in the vocabulary')
                continue

            # accessing the output embedding of each of the compounds present in the papers published until year y:
            for compound in all_compounds_from_ner:
                print('Accessing the output embedding of {}'.format(compound))
                compound_we_comb16 = get_w2v_output_embedding(compound, model_comb16, method='da')
                if compound_we_comb16 is None:
                    print(f"Compound '{compound}' not found in the model's vocabulary.")
                    continue

                dot_product_comb16 = np.dot(compound_we_comb16, target_disease_we).item()

                dictionary_for_all_compounds['{}_comb16'.format(compound)]['year'].append(y)
                dictionary_for_all_compounds['{}_comb16'.format(compound)]['dot_product_result'].append(
                    dot_product_comb16)
                dictionary_for_all_compounds['{}_comb16'.format(compound)][
                    'dot_product_result_absolute'].append(abs(dot_product_comb16))

    print('Generating historical record for each compound')
    for c in all_compounds_from_ner:
        key = f'{c}_comb{combination}'
        key_filename = re.sub(r'[\\/*?:"<>|]', '_', key)

        # Verifique se a chave existe e se os dados não estão vazios antes de processar
        if key not in dictionary_for_all_compounds:
            print(f"Aviso: Chave '{key}' não encontrada no dicionário. Pulando.")
            continue

        # Pega a lista de produtos escalares para facilitar o acesso
        dot_products_abs = dictionary_for_all_compounds[key]['dot_product_result_absolute']

        # --- VERIFICAÇÃO PRINCIPAL: Garante que a lista de dados não está vazia ---
        if not dot_products_abs:
            print(f"Aviso: Lista de dados para '{key}' está vazia. Pulando cálculos.")
            # Opcional: Você pode querer escrever um CSV vazio ou simplesmente pular
            # Aqui, vamos pular a escrita do arquivo para evitar arquivos vazios.
            continue

        # --- Cálculos com segurança ---

        # Softmax:
        # A verificação 'if not dot_products_abs:' já previne o erro aqui.
        dictionary_for_all_compounds[key]['softmax'] = softmax(dot_products_abs)

        # Normalization:
        maximum = np.max(dot_products_abs)
        if maximum > 0:
            normalized_values = [x / maximum for x in dot_products_abs]
            dictionary_for_all_compounds[key]['normalized_dot_product_absolute'] = normalized_values
            dictionary_for_all_compounds[key]['softmax_normalization'] = softmax(normalized_values)
        else:
            # Lida com o caso em que todos os valores são 0, evitando divisão por zero.
            dictionary_for_all_compounds[key]['normalized_dot_product_absolute'] = [0.0] * len(dot_products_abs)
            dictionary_for_all_compounds[key]['softmax_normalization'] = softmax([0.0] * len(dot_products_abs))

        # Standardization:
        # A padronização requer pelo menos 2 pontos para ter um desvio padrão > 0.
        if len(dot_products_abs) > 1:
            mean = np.mean(dot_products_abs)
            standard_deviation = np.std(dot_products_abs)

            if standard_deviation > 0:
                standardized_values = [(x - mean) / standard_deviation for x in dot_products_abs]
                dictionary_for_all_compounds[key]['standardized_dot_product_absolute'] = standardized_values
                dictionary_for_all_compounds[key]['softmax_standardization'] = softmax(standardized_values)
            else:
                # Caso raro em que todos os valores são idênticos, o desvio padrão é 0.
                dictionary_for_all_compounds[key]['standardized_dot_product_absolute'] = [0.0] * len(dot_products_abs)
                dictionary_for_all_compounds[key]['softmax_standardization'] = softmax([0.0] * len(dot_products_abs))
        else:
            # Se houver apenas um valor, a padronização não é significativa (resultaria em desvio padrão 0).
            dictionary_for_all_compounds[key]['standardized_dot_product_absolute'] = [0.0]
            dictionary_for_all_compounds[key]['softmax_standardization'] = softmax([0.0])

        print(f'Writing file for {key}')
        pd.DataFrame.from_dict(data=dictionary_for_all_compounds[key]).to_csv(
            f'{validation_directory_path}/{key_filename}.csv',
            columns=['year', 'dot_product_result', 'dot_product_result_absolute', 'softmax',
                     'normalized_dot_product_absolute', 'standardized_dot_product_absolute',
                     'softmax_normalization', 'softmax_standardization'],
            index=False
        )

    # OSError: Cannot save file into a non-existent directory: 'data/acute_myeloid_leukemia/validation/per_compound/w2v/chemotherapy'

    print('END!')