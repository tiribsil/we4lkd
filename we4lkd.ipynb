{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui eu vou escrever todo o programa e explicar cada parte :)\n",
    "\n",
    "Primeiro, vou apenas copiar os blocos de código do projeto do Matheus e escrever o que precisa ser alterado."
   ],
   "id": "9cff3992a802919d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Crawler:\n",
    "\n",
    "Aqui serão extraídos os nomes e _abstracts_ dos artigos relacionados aos termos de busca."
   ],
   "id": "6ee4d97548b1fce3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704b06d-2b21-44e5-a224-d68d2dd8deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, string, re, os, fnmatch\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Essas são funções auxiliares para o funcionamento do _crawler_.",
   "id": "8276a1fe101cfd5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def list_from_txt(file_path):\n",
    "    strings_list = []\n",
    "    with open(file_path, 'rt', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            strings_list.append(line.rstrip('\\n'))\n",
    "    return strings_list\n",
    "\n",
    "def search(paper_query):\n",
    "    final_query = '{} AND English[Language]'.format(paper_query)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax='999999',\n",
    "                            retmode='xml',\n",
    "                            term=final_query)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def fetch_details(paper_ids):\n",
    "    ids_string = ','.join(paper_ids)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids_string)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def flat(round_list):\n",
    "    flat_list = []\n",
    "    for element in round_list:\n",
    "        if type(element) is list:\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list"
   ],
   "id": "377fc3723cfd14b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Essa ultima função auxiliar é a que insere aos termos de busca sinônimos de compostos para aumentar o escopo da busca.\n",
    "\n",
    "ALTERAÇÕES NECESSÁRIAS: Aqui está _hardcoded_ os IDs dos compostos que terão seus sinônimos inclusos. Esses compostos são tratamentos conhecidos para a AML, e foram levantados manualmente por um especialista.\n",
    "\n",
    "SOLUÇÕES?: Preciso substituir esses compostos manualmente escolhidos, arrumando alguma forma de obter esses IDs automaticamente para uma doença escolhida. Como? Não sei ainda."
   ],
   "id": "1ea5a37325408cc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extend_search():\n",
    "    cids = [\n",
    "        14888, 9444, 62770, 2907, 6253, 122640033, 5743, 90480031, 76970819, 636362, 71657455, 9829523, 51082, 5865,\n",
    "        2723601, 249332, 49846579, 11422859\n",
    "    ]\n",
    "\n",
    "    sids = [\n",
    "        404336834\n",
    "    ]\n",
    "\n",
    "    extended = []\n",
    "    extended.append(pcp.Compound.from_cid(cids[0]).synonyms)\n",
    "\n",
    "    for c in cids[1:]:\n",
    "        extended.extend(pcp.Compound.from_cid(c).synonyms)\n",
    "\n",
    "    for s in sids:\n",
    "        extended.extend(pcp.Substance.from_sid(s).synonyms)\n",
    "\n",
    "    extended = flat(extended)\n",
    "    remove_words = []\n",
    "    for s in extended:\n",
    "        if '[as the base]' in s or '[Poison]' in s or '[ISO]' in s or '[INN]' in s or 'USP/JAN' in s or '(JAN' in s or '[JAN' in s or 'Latin' in s or 'Spanish' in s or '(TN)' in s or '(INN)' in s or 'USAN' in s or 'JP17/USP' in s or 'Czech' in s or 'German' in s or '[CAS]' in s:\n",
    "            remove_words.append(s)\n",
    "\n",
    "    extended = [x for x in extended if x not in remove_words]\n",
    "    extended = [x for x in extended if len(x) >= 3]\n",
    "    extended = list(dict.fromkeys(extended))\n",
    "\n",
    "    useless_strings = list_from_txt('./data/bad_search_strings.txt')\n",
    "    for w in useless_strings:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    remove_words = ['Antibiotic U18496', 'D,L-Cyclophosphamide', 'N,O-propylen-phosphorsaeure-ester-diamid',\n",
    "                    'UNII-6UXW23996M component CMSMOCZEIVJLDB-CQSZACIVSA-N']\n",
    "    for w in remove_words:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return extended"
   ],
   "id": "705c53058a8103fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui é o programa principal do _crawler_. Rodar isso aqui demora muito, então não sei ainda como vou testar.\n",
    "\n",
    "ALTERAÇÕES NECESSÁRIAS:\n",
    "\n",
    "   1. Termos de busca não são fixos, preciso de uma função auxiliar que retorna termos de busca para a doença de escolha.\n",
    "\n",
    "   2. Parecido com o 1, mas para os sinônimos.\n",
    "\n",
    "   3."
   ],
   "id": "a42bd370c491a485"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    destination_path = './results/'\n",
    "    Path('./data/ids.txt').touch(exist_ok=True)\n",
    "    Path(destination_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Cria uma lista com todos os termos de busca do arquivo. (não vai mais existir esse arquivo)\n",
    "    queries = list_from_txt('./data/queries.txt')\n",
    "    paper_counter = 0\n",
    "\n",
    "    # Insere os sinônimos dos compostos no fim da lista. (posso mudar para uma chamada de método)\n",
    "    synonyms = extend_search()\n",
    "    queries.extend(synonyms)\n",
    "\n",
    "    # Cria uma lista com os IDs de todos os artigos já obtidos e um conjunto de IDs de artigos obtidos.\n",
    "    old_papers = list_from_txt('./data/ids.txt')\n",
    "    ids = set(old_papers)\n",
    "\n",
    "    # Para cada termo de busca...\n",
    "    for query in queries[100:]:\n",
    "        # Normaliza o termo de busca.\n",
    "        query = query.encode('ascii', 'ignore').decode('ascii')\n",
    "        print('searching for {}'.format(query))\n",
    "\n",
    "        # Procura pelo termo de busca, salva os IDs dos artigos encontrados em id_list.\n",
    "        id_list = list(search(query)['IdList'])\n",
    "        # Mantém só os que ainda não estão no conjunto de IDs conhecidos, ou seja, os novos.\n",
    "        id_list = [x for x in id_list if x not in ids]\n",
    "\n",
    "        # Se não achar nada novo, pode ir para o próximo termo de busca.\n",
    "        if not id_list:\n",
    "            print('No new papers found\\n')\n",
    "            continue\n",
    "\n",
    "        print('{} papers found\\n'.format(len(id_list)))\n",
    "        # Insere os novos IDs no conjunto.\n",
    "        ids.update(id_list)\n",
    "\n",
    "        # Pega os detalhes de cada artigo novo.\n",
    "        papers = fetch_details(id_list)\n",
    "\n",
    "        # Cria uma pasta com o nome do termo de busca, formatado para poder ser nome de pasta.\n",
    "        folder_name = query.lower().translate(str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "        Path(destination_path + '{}'.format(folder_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Para cada artigo novo...\n",
    "        for paper in papers['PubmedArticle']:\n",
    "            article_title = ''\n",
    "            article_title_filename = ''\n",
    "            article_abstract = ''\n",
    "            article_year = ''\n",
    "            filename = ''\n",
    "            path_name = ''\n",
    "\n",
    "            try:\n",
    "                article_title = paper['MedlineCitation']['Article']['ArticleTitle']\n",
    "                article_title_filename = article_title.lower().translate(\n",
    "                    str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "            except KeyError as e: continue\n",
    "\n",
    "            try:\n",
    "                article_abstract = ' '.join(paper['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "                article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "\n",
    "            except KeyError as e:\n",
    "                if 'Abstract' in e.args:  # caso o artigo não tenha prefácio, continua o processamento, pois o título já foi extraído\n",
    "                    article_abstract = ''\n",
    "                    pass\n",
    "\n",
    "                elif 'Year' in e.args:\n",
    "                    article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate'][\n",
    "                                       'MedlineDate'][0:4]\n",
    "\n",
    "            if len(article_year) == 4:\n",
    "                filename = '{}_{}'.format(article_year, article_title_filename)\n",
    "\n",
    "                if len(filename) > 150:\n",
    "                    filename = filename[0:146]\n",
    "\n",
    "                path_name = destination_path + folder_name + '/{}.txt'.format(filename)\n",
    "                path_name = path_name.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "                # depois de pegar o título, resumo e data (e pulando o loop, quando não é possível), escrever o arquivo:\n",
    "                with open(path_name, \"a\", encoding='utf-8') as myfile:\n",
    "                    myfile.write(article_title + ' ' + article_abstract)\n",
    "\n",
    "                paper_counter += 1\n",
    "\n",
    "        with open('./data/ids.txt', 'a+', encoding='utf-8') as f:\n",
    "            for new_id in id_list:\n",
    "                f.write('\\n' + str(new_id))\n",
    "\n",
    "    print('Crawler finished with {} papers collected.'.format(len(old_papers) + paper_counter))"
   ],
   "id": "a90551a1b1098c7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
