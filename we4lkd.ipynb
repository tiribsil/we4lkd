{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui eu vou escrever todo o programa e explicar cada parte :)\n",
    "\n",
    "Primeiro, vou apenas copiar os blocos de código do projeto do Matheus e escrever o que precisa ser alterado.\n",
    "\n",
    "Uma coisa que tenho percebido ao redor do código é a falta de funções auxiliares. Eu pessoalmente não gosto de main longa e cheia, então acho que vou ajustar isso depois."
   ],
   "id": "f9d66732fbf1f68c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Crawler:\n",
    "\n",
    "Aqui serão extraídos os nomes e _abstracts_ dos artigos relacionados aos termos de busca."
   ],
   "id": "6ee4d97548b1fce3"
  },
  {
   "cell_type": "code",
   "id": "0704b06d-2b21-44e5-a224-d68d2dd8deed",
   "metadata": {},
   "source": [
    "import string, os\n",
    "\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Essas são funções auxiliares para o funcionamento do _crawler_.",
   "id": "8276a1fe101cfd5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def list_from_txt(file_path):\n",
    "    strings_list = []\n",
    "    with open(file_path, 'rt', encoding='utf-8') as list_file:\n",
    "        for line in list_file: strings_list.append(line.rstrip('\\n'))\n",
    "    return strings_list\n",
    "\n",
    "def search(paper_query):\n",
    "    final_query = '{} AND English[Language]'.format(paper_query)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax='999999',\n",
    "                            retmode='xml',\n",
    "                            term=final_query)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def fetch_details(paper_ids):\n",
    "    ids_string = ','.join(paper_ids)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids_string)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def flat(round_list):\n",
    "    flat_list = []\n",
    "    for element in round_list:\n",
    "        if type(element) is list:\n",
    "            for item in element: flat_list.append(item)\n",
    "        else: flat_list.append(element)\n",
    "    return flat_list"
   ],
   "id": "377fc3723cfd14b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Essa ultima função auxiliar é a que insere aos termos de busca sinônimos de compostos para aumentar o escopo da busca.\n",
    "\n",
    "ALTERAÇÕES NECESSÁRIAS: aqui está _hardcoded_ os IDs dos compostos que terão seus sinônimos inclusos. Esses compostos são tratamentos conhecidos para a AML, e foram levantados manualmente por um especialista.\n",
    "\n",
    "SOLUÇÕES?: preciso substituir esses compostos manualmente escolhidos, arrumando alguma forma de obter esses IDs automaticamente para uma doença escolhida. Como? Não sei ainda."
   ],
   "id": "1ea5a37325408cc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extend_search():\n",
    "    compound_ids = [\n",
    "        14888, 9444, 62770, 2907, 6253, 122640033, 5743, 90480031, 76970819, 636362, 71657455, 9829523, 51082, 5865,\n",
    "        2723601, 249332, 49846579, 11422859\n",
    "    ]\n",
    "\n",
    "    substance_ids = [\n",
    "        404336834\n",
    "    ]\n",
    "\n",
    "    extended = [pcp.Compound.from_cid(compound_ids[0]).synonyms]\n",
    "\n",
    "    for c in compound_ids[1:]:\n",
    "        extended.extend(pcp.Compound.from_cid(c).synonyms)\n",
    "\n",
    "    for s in substance_ids:\n",
    "        extended.extend(pcp.Substance.from_sid(s).synonyms)\n",
    "\n",
    "    extended = flat(extended)\n",
    "    remove_words = []\n",
    "    for s in extended:\n",
    "        if '[as the base]' in s or '[Poison]' in s or '[ISO]' in s or '[INN]' in s or 'USP/JAN' in s or '(JAN' in s or '[JAN' in s or 'Latin' in s or 'Spanish' in s or '(TN)' in s or '(INN)' in s or 'USAN' in s or 'JP17/USP' in s or 'Czech' in s or 'German' in s or '[CAS]' in s:\n",
    "            remove_words.append(s)\n",
    "\n",
    "    extended = [x for x in extended if x not in remove_words]\n",
    "    extended = [x for x in extended if len(x) >= 3]\n",
    "    extended = list(dict.fromkeys(extended))\n",
    "\n",
    "    useless_strings = list_from_txt('./data/bad_search_strings.txt')\n",
    "    for w in useless_strings:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    remove_words = ['Antibiotic U18496', 'D,L-Cyclophosphamide', 'N,O-propylen-phosphorescence-ester-diamid',\n",
    "                    'UNII-6UXW23996M component CMSMOCZEIVJLDB-CQSZACIVSA-N']\n",
    "    for w in remove_words:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return extended"
   ],
   "id": "705c53058a8103fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui é o programa principal do _crawler_. Rodar isso aqui demora muito, então não sei ainda como vou testar.\n",
    "\n",
    "ALTERAÇÕES NECESSÁRIAS:\n",
    "\n",
    "   1. Termos de busca não são fixos, preciso de uma função auxiliar que retorna termos de busca para a doença de escolha.\n",
    "\n",
    "   2. Parecido com o 1, mas para os sinônimos.\n",
    "\n",
    "   3. Não acho que esse arquivo de IDs vai continuar existindo."
   ],
   "id": "a42bd370c491a485"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    destination_path = './results/'\n",
    "    Path('./data/ids.txt').touch(exist_ok=True)\n",
    "    Path(destination_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Cria uma lista com todos os termos de busca do arquivo. (não vai mais existir esse arquivo)\n",
    "    queries = list_from_txt('./data/queries.txt')\n",
    "    paper_counter = 0\n",
    "\n",
    "    # Insere os sinônimos dos compostos no fim da lista.\n",
    "    synonyms = extend_search()\n",
    "    queries.extend(synonyms)\n",
    "\n",
    "    # Cria uma lista com os IDs de todos os artigos já obtidos e um conjunto de IDs de artigos obtidos.\n",
    "    old_papers = list_from_txt('./data/ids.txt')\n",
    "    ids = set(old_papers)\n",
    "\n",
    "    # Para cada termo de busca...\n",
    "    for query in queries[100:]:\n",
    "        # Normaliza o termo de busca.\n",
    "        query = query.encode('ascii', 'ignore').decode('ascii')\n",
    "        print('searching for {}'.format(query))\n",
    "\n",
    "        # Procura pelo termo de busca, salva os IDs dos artigos encontrados em id_list.\n",
    "        id_list = list(search(query)['IdList'])\n",
    "        # Mantém só os que ainda não estão no conjunto de IDs conhecidos, ou seja, os novos.\n",
    "        id_list = [x for x in id_list if x not in ids]\n",
    "\n",
    "        # Se não achar nada novo, pode ir para o próximo termo de busca.\n",
    "        if not id_list:\n",
    "            print('No new papers found\\n')\n",
    "            continue\n",
    "\n",
    "        print('{} papers found\\n'.format(len(id_list)))\n",
    "        # Insere os novos IDs no conjunto.\n",
    "        ids.update(id_list)\n",
    "\n",
    "        # Pega os detalhes de cada artigo novo.\n",
    "        papers = fetch_details(id_list)\n",
    "\n",
    "        # Cria uma pasta com o nome do termo de busca, formatado para poder ser nome de pasta.\n",
    "        folder_name = query.lower().translate(str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "        Path(destination_path + '{}'.format(folder_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Para cada artigo novo...\n",
    "        for paper in papers['PubmedArticle']:\n",
    "            article_year = ''\n",
    "            article_abstract = ''\n",
    "\n",
    "            # Se não tiver título, vai para o próximo\n",
    "            try:\n",
    "                article_title = paper['MedlineCitation']['Article']['ArticleTitle']\n",
    "                article_title_filename = article_title.lower().translate(\n",
    "                    str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "            except KeyError as e: continue\n",
    "\n",
    "            # Se não tiver prefácio, vai para o próximo.\n",
    "            try:\n",
    "                article_abstract = ' '.join(paper['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "                article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "            except KeyError as e:\n",
    "                if 'Abstract' in e.args: continue\n",
    "                if 'Year' in e.args:\n",
    "                    article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate'][\n",
    "                                       'MedlineDate'][0:4]\n",
    "\n",
    "            # Se não tiver ano válido, vai para o próximo.\n",
    "            if len(article_year) != 4: continue\n",
    "\n",
    "            # O nome do arquivo vai ser \"{ano}_{nome_do_artigo}\"...\n",
    "            filename = '{}_{}'.format(article_year, article_title_filename)\n",
    "            if len(filename) > 150: filename = filename[0:146]\n",
    "\n",
    "            # e vai ser escrito naquela pasta criada antes do loop.\n",
    "            path_name = destination_path + folder_name + '/{}.txt'.format(filename)\n",
    "            path_name = path_name.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "            # Escreve o arquivo.\n",
    "            with open(path_name, \"a\", encoding='utf-8') as file:\n",
    "                file.write(article_title + ' ' + article_abstract)\n",
    "\n",
    "            paper_counter += 1\n",
    "\n",
    "        # Coloca os IDs dos artigos novos no arquivo.\n",
    "        with open('./data/ids.txt', 'a+', encoding='utf-8') as file:\n",
    "            for new_id in id_list: file.write('\\n' + str(new_id))\n",
    "\n",
    "    print('Crawler finished with {} papers collected.'.format(len(old_papers) + paper_counter))"
   ],
   "id": "a90551a1b1098c7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Agregação dos textos por ano\n",
    "\n",
    "Após pegar todos os prefácios resultantes das buscas, os textos precisam ser agrupados por ano para realizar aquela análise de quão antes o tratamento foi descoberto."
   ],
   "id": "5099efda3cae7e27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    start_year = 1900\n",
    "    end_year = 2025\n",
    "    source_path = './results/'\n",
    "    destination_path = './results_aggregated/'\n",
    "\n",
    "    # Pega os nomes de todos os arquivos que vieram do crawler.\n",
    "    filenames = list(map(str, Path(source_path).glob('**/*.txt')))\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    # Para cada ano no intervalo...\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Pega os nomes dos arquivos de artigos que são do ano atual.\n",
    "        filenames_current_year = [f for f in filenames if Path(f).stem.startswith(str(year))]\n",
    "\n",
    "        # Se não tiver nenhum desse ano, vai para o próximo.\n",
    "        if not filenames_current_year:\n",
    "            continue\n",
    "\n",
    "        print(f'{len(filenames_current_year)} papers from {year}.\\n')\n",
    "\n",
    "        # Pega os prefácios de todos os artigos desse ano.\n",
    "        abstracts = []\n",
    "        for fname in filenames:\n",
    "            with open(fname, encoding='utf-8') as infile:\n",
    "                abstracts.extend(infile.readlines())\n",
    "\n",
    "        # Junta tudo em um só arquivo texto.\n",
    "        output_file = Path(destination_path) / f'results_file_1900_{year}.txt'\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for fname, abstract in zip(filenames, abstracts):\n",
    "                f.write(f\"{Path(fname).stem[5:]}|{abstract}\")"
   ],
   "id": "a678a4e378250ac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Pré-processamento dos textos\n",
    "\n",
    "Aqui os textos serão traduzidos em DataFrames para melhorar o desempenho do processamento."
   ],
   "id": "421fac54a0c036aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from functools import reduce"
   ],
   "id": "14fec3dc0ada7d1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As funções auxiliares são altamente customizadas para a LMA, muitas coisas vão ter que mudar.",
   "id": "610428f083d65c24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def ss():\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    return SparkSession(sc)\n",
    "\n",
    "def dataframes_from_txt(summaries_path):\n",
    "    filenames = sorted([str(x) for x in Path(summaries_path).glob('*.txt')])\n",
    "    dataframes = []\n",
    "\n",
    "    # Para cada arquivo no diretório...\n",
    "    for file_path in filenames:\n",
    "        # Pega o ano.\n",
    "        year_of_file = file_path\\\n",
    "            .replace(os.path.join(summaries_path, 'results_file_1900_'), '')\\\n",
    "            .replace('.txt', '')\n",
    "\n",
    "        NATURE_FILTERED_WORDS_IN_TITLE = [\n",
    "            'foreword', 'prelude', 'commentary', 'workshop', 'conference', 'symposium',\n",
    "            'comment', 'retract', 'correction', 'erratum', 'memorial'\n",
    "        ]\n",
    "\n",
    "        # Filtra os artigos com título contendo pelo menos uma dessas palavras.\n",
    "        title_doesnt_have_nature_filtered_words = reduce(\n",
    "            lambda acc, word: acc & (F.locate(word, F.col('title')) == F.lit(0)),\n",
    "            NATURE_FILTERED_WORDS_IN_TITLE,\n",
    "            F.lit(True)\n",
    "        )\n",
    "\n",
    "        # Cria uma tabela para cada arquivo com as colunas filename, title, summary e id.\n",
    "        df = ss()\\\n",
    "            .read\\\n",
    "            .option('header', 'false')\\\n",
    "            .option('lineSep', '\\n')\\\n",
    "            .option('sep', '|')\\\n",
    "            .option('quote', '')\\\n",
    "            .csv(file_path)\\\n",
    "            .withColumn('filename', F.lit(year_of_file))\\\n",
    "            .withColumnRenamed('_c0', 'title')\\\n",
    "            .withColumnRenamed('_c1', 'summary')\\\n",
    "            .where(title_doesnt_have_nature_filtered_words)\\\n",
    "            .withColumn('id', F.monotonically_increasing_id())\n",
    "\n",
    "        # Coloca essa tabela na lista de tabelas.\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Junta tudo em uma tabela só e retorna.\n",
    "    return reduce(lambda df1, df2: df1.union(df2), dataframes)\n",
    "\n",
    "def get_csv_in_folder(file_path):\n",
    "    \"\"\"Search for a .csv file in a given path. It must find just one .csv file - this constraint is tested with assert command.\n",
    "    This is an auxiliar function used during reading .csv PySpark DataFrames.\n",
    "    Args:\n",
    "        file_path: path to the folder containg the .csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    files = os.listdir(file_path)\n",
    "    files = filter(lambda x: x[-3:] == 'csv', files)\n",
    "    files = list(files)\n",
    "\n",
    "    assert len(files) == 1, files\n",
    "\n",
    "    return os.path.join(file_path, files[0])\n",
    "\n",
    "def read_csv_table_files(file_path, sep=','):\n",
    "    full_path = file_path\n",
    "\n",
    "    if file_path[-3:] != 'csv':\n",
    "        file_path = get_csv_in_folder(file_path)\n",
    "\n",
    "    return ss()\\\n",
    "        .read\\\n",
    "        .option('header', 'true')\\\n",
    "        .option('sep', sep)\\\n",
    "        .csv(full_path)\n",
    "\n",
    "def to_csv(df, target_folder, num_files=1, sep=','):\n",
    "    \"\"\"Saves a PySpark Dataframe into .csv file.\n",
    "    Args:\n",
    "        df: object of the DataFrame;\n",
    "        target_folder: path where the .csv is going to be saved;\n",
    "        num_files: number of .csv files to be created, default is 1.\n",
    "    \"\"\"\n",
    "\n",
    "    return df\\\n",
    "        .coalesce(num_files)\\\n",
    "        .write\\\n",
    "        .mode('overwrite')\\\n",
    "        .option('header', 'true')\\\n",
    "        .option('sep', sep)\\\n",
    "        .format('csv')\\\n",
    "        .save(target_folder)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Returns WORDNET POS compliance to WORDNET lemmatization (ADJ, VERB, NOUN, ADV)\"\"\"\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "            return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "            return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "            return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "            return 'r'\n",
    "    else:\n",
    "    # As default pos in lemmatization is Noun\n",
    "        return 'n'\n",
    "\n",
    "def tokenize(data):\n",
    "    \"\"\"Toeknizes a sentence\n",
    "\n",
    "        Args:\n",
    "        data: a sentence (string).\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return ['']\n",
    "\n",
    "    else:\n",
    "        return word_tokenize(data)\n",
    "\n",
    "def remove_last_digit(data):\n",
    "    \"\"\"Removes the last character of a string a sentence\n",
    "\n",
    "        Args:\n",
    "        data: a string.\n",
    "    \"\"\"\n",
    "\n",
    "    return data[:-1]\n",
    "\n",
    "def return_last_digit(data):\n",
    "    \"\"\"Toeknizes a sentence\n",
    "\n",
    "        Args:\n",
    "        data: a sentence (string).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        aux = data[-1]\n",
    "\n",
    "    except:\n",
    "        aux = ''\n",
    "\n",
    "    return aux\n",
    "\n",
    "def summary_column_preprocessing(column, bert_model=False):\n",
    "    \"\"\"Executes intial preprocessing in a PySpark text column. It removes some unwanted regex from the text.\n",
    "    Args:\n",
    "        column: the name of the column to be processed.\n",
    "    \"\"\"\n",
    "\n",
    "    aml_synonyms = [\n",
    "        'acute[- ]?myeloid[- ]?leukemia',\n",
    "        'acute myelocytic leukemia',\n",
    "        'acute myelogenous leukemia',\n",
    "        'acute granulocytic leukemia',\n",
    "        'acute non-lymphocytic leukemia',\n",
    "        'acute mylogenous leukemia',\n",
    "        'acute myeloid leukemia',\n",
    "        'acute nonlymphoblastic leukemia',\n",
    "        'acute myeloblastic leukemia',\n",
    "\n",
    "        # subtipos de AML:\n",
    "        'acute erythroid leukemia',\n",
    "        'acute myelomonocytic leukemia',\n",
    "        'acute monocytic leukemia',\n",
    "        'acute megakaryoblastic leukemia',\n",
    "        'acute promyelocytic leukemia',\n",
    "    ]\n",
    "\n",
    "    regex = r'(?i)({})'.format('|'.join(aml_synonyms))\n",
    "\n",
    "    column = F.trim(column)\n",
    "\n",
    "    column = F.regexp_replace(column, r'<[^>]+>', '')\n",
    "    column = F.regexp_replace(column, r'([--:\\w?@%&+~#=]*\\.[a-z]{2,4}\\/{0,2})((?:[?&](?:\\w+)=(?:\\w+))+|[--:\\w?@%&+~#=]+)?', '')\n",
    "\n",
    "    if bert_model == False:\n",
    "        column = F.regexp_replace(column, r'[;:\\(\\)\\[\\]\\{\\}.,\"!#$&\\'*?@\\\\\\^`|~]', '')\n",
    "\n",
    "    column = F.regexp_replace(column, r'\\s+', ' ')\n",
    "    column = F.regexp_replace(column, r'(?i)(leukaemia)', 'leukemia')\n",
    "    column = F.regexp_replace(column, r'(?i)(leukaemic)', 'leukemic')\n",
    "\n",
    "    column = F.regexp_replace(column, regex, 'AML')\n",
    "    column = F.regexp_replace(column, regex, 'AML')\n",
    "\n",
    "    column = F.regexp_replace(column, r'(?i)(acute myeloid or (lymphoblastic|lymphoid) leukemia[s]?)', 'AML or ALL')\n",
    "    column = F.regexp_replace(column, r'(?i)(acute myeloid and (lymphoblastic|lymphoid) leukemia[s]?)', 'AML and ALL')\n",
    "    column = F.regexp_replace(column, r'(?i)(acute myeloid and chronic lymphocytic leukemia[s]?)', 'AML and CLL')\n",
    "\n",
    "    # cytarabine\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside triphosphate )', ' cytarabinetriphosphate ')\n",
    "    column = F.regexp_replace(column, r'(?i)(cytosine arabinoside triphosphate )', 'cytarabinetriphosphate ')\n",
    "    column = F.regexp_replace(column, r'(?i)(\\(cytosine arabinoside triphosphate\\))', '(cytarabinetriphosphate)')\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside triphosphate.)', ' cytarabinetriphosphate.')\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside triphosphate,)', ' cytarabinetriphosphate,')\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside monophosphate )', ' cytarabinemonophosphate')\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside monophosphate.)', ' cytarabinemonophosphate.')\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside monophosphate,)', ' cytarabinemonophosphate,')\n",
    "    column = F.regexp_replace(column, r\" arabinocytidine 5' phosphate|arabinofuranosylcytosine 5'-triphosphate|Ara-CTP\", \" cytarabine5phosphate\")\n",
    "    column = F.regexp_replace(column, r\"(?i)(1-beta-D|1-β-d|1β-d|1 beta-D)-Arabinofuranosylcytosine 5'-triphosphate\", \"cytarabine5phosphate\")\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside | \\[Ara-C\\] )', ' cytarabine ')\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside\\.)', ' cytarabine.')\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside,)', ' cytarabine,')\n",
    "    column = F.regexp_replace(column, r'(?i)( cytosine arabinoside:)', ' cytarabine:')\n",
    "    column = F.regexp_replace(column, r'(?i)(\\(cytosine arabinoside\\))', '(cytarabine)')\n",
    "    column = F.regexp_replace(column, r'(?i)(\\(ara[-]?c\\))', '(cytarabine)')\n",
    "    column = F.regexp_replace(column, r'(?i)(\\(ara[-]?c, )', '(cytarabine, ')\n",
    "    column = F.regexp_replace(column, r'(?i)(\\(ara[-]?c )', '(cytarabine ')\n",
    "    column = F.regexp_replace(column, r'(?i)( ara[-]?c\\))', ' cytarabine)')\n",
    "    column = F.regexp_replace(column, r'(?i)( ara[-]?c/)', ' cytarabine/)')\n",
    "    column = F.regexp_replace(column, r'(?i)(/ara[-]?c\\.)', '/cytarabine.')\n",
    "    column = F.regexp_replace(column, r'(?i)(/ara[-]?c )', '/cytarabine ')\n",
    "    column = F.regexp_replace(column, r'(?i)(ara[-]?c-induced)', 'cytarabine-induced')\n",
    "    column = F.regexp_replace(column, r'(?i)(ara[-]?c-treated)', 'cytarabine-treated')\n",
    "    column = F.regexp_replace(column, r'(?i)(ara[-]?c-based)', 'cytarabine-based')\n",
    "    column = F.regexp_replace(column, r'(?i)(ara[-]?c-resistant)', 'cytarabine-resistant')\n",
    "    column = F.regexp_replace(column, r'(?i)(/ara[-]?c/)', '/cytarabine/')\n",
    "    column = F.regexp_replace(column, r'(HDAra-C|HD-Ara-C)', 'high-dose cytarabine')\n",
    "    column = F.regexp_replace(column, r'(LDAra-C|LD-Ara-C|LDAC)', 'low-dose cytarabine')\n",
    "    column = F.regexp_replace(column, r'(IDAra-C|ID-Ara-C)', 'intermediate-dose cytarabine')\n",
    "    column = F.regexp_replace(column, r'\\(Ara-C;', '(cytarabine;')\n",
    "    column = F.regexp_replace(column, r'\\[3H\\]Ara-C', '([3H]cytarabine')\n",
    "    column = F.regexp_replace(column, r' Arabinocytidine', ' Cytarabine')\n",
    "    column = F.regexp_replace(column, r\"(?i)(1-beta-D|1-β-d|1β-d|1 beta-D)-Arabinofuranosyl[-]?cytosine \", \"cytarabine \")\n",
    "    column = F.regexp_replace(column, r\"(?i)(1-beta-D|1-β-d|1β-d|1 beta-D)-Arabinofuranosyl[-]?cytosine,\", \"cytarabine,\")\n",
    "    column = F.regexp_replace(column, r\"(?i)(1-beta-D|1-β-d|1β-d|1 beta-D)-Arabinofuranosyl[-]?cytosine.\", \"cytarabine.\")\n",
    "    column = F.regexp_replace(column, r'(?i)( Arabinofuranosylcytosine | Ara-C )', \" cytarabine \")\n",
    "    column = F.regexp_replace(column, r'(?i)( Arabinofuranosylcytosine,)', \" cytarabine,\")\n",
    "    column = F.regexp_replace(column, r'(?i)( Arabinofuranosylcytosine\\.)', \" cytarabine.\")\n",
    "    column = F.regexp_replace(column, r' arabinocytidine ', ' cytarabine ')\n",
    "    column = F.regexp_replace(column, r' arabinocytidine,', ' cytarabine,')\n",
    "    column = F.regexp_replace(column, r' arabinocytidine:', ' cytarabine:')\n",
    "    column = F.regexp_replace(column, r' arabinocytidine\\.', ' cytarabine.')\n",
    "    column = F.regexp_replace(column, r'147-94-4', 'cytarabine')\n",
    "    column = F.regexp_replace(column, r'\\(Cytosar-U\\)|\\(Cytosar\\)', '(cytarabine)')\n",
    "    column = F.regexp_replace(column, r'(?i)\\(Cytosar-U,', '(cytarabine,')\n",
    "    column = F.regexp_replace(column, r'(?i) cytosar\\)', ' cytarabine)')\n",
    "    column = F.regexp_replace(column, r' aracytin[e]? ', ' cytarabine ')\n",
    "    column = F.regexp_replace(column, r' aracytin[e]?\\.', ' cytarabine.')\n",
    "    column = F.regexp_replace(column, r' aracytin[e]?,', ' cytarabine,')\n",
    "    column = F.regexp_replace(column, r'\\+aracytine\\+', '+cytarabine+')\n",
    "    column = F.regexp_replace(column, r'-aracytine ', '-cytarabine ')\n",
    "    column = F.regexp_replace(column, r'-aracytine\\.', '-cytarabine.')\n",
    "    column = F.regexp_replace(column, r'-aracytine:', '-cytarabine:')\n",
    "    column = F.regexp_replace(column, r'(?i)(Cytosine beta-D-arabinoside)', 'cytarabine')\n",
    "    column = F.regexp_replace(column, r'(?i)(Cytosine-1-beta-D-arabinofuranoside)', 'cytarabine')\n",
    "    column = F.regexp_replace(column, r'(?i)(beta-ara c)', 'cytarabine')\n",
    "    column = F.regexp_replace(column, r'(?i)(liposomal cytarabine)', 'liposomalcytarabine')\n",
    "    column = F.regexp_replace(column, r'(?i)(depocyte)', 'liposomalcytarabine')\n",
    "    column = F.regexp_replace(column, r\"4'-thio-ara-C\", \"4-thio-cytarabine\")\n",
    "    column = F.regexp_replace(column, r\"NSC[ -]63878\", \"cytarabine\")\n",
    "    column = F.regexp_replace(column, r\"ofcytarabine\", \"of cytarabine\")\n",
    "\n",
    "    # daunorubicin:\n",
    "    column = F.regexp_replace(column, r'[Dd]aunomycin \\(DAU\\)|[Dd]aunorubicin \\(DAU\\)|daunorubicin hydrochloride \\(DAU\\)', 'daunorubicin (daunorubicin)')\n",
    "    column = F.regexp_replace(column, r'NSC[ -]?82151|Rubomycin C|[^R]DNX[^B]', 'daunorubicin')\n",
    "    column = F.regexp_replace(column, r'daunomycin-', 'daunorubicin-')\n",
    "    column = F.regexp_replace(column, r'Daunomycin-', 'Daunorubicin-')\n",
    "    column = F.regexp_replace(column, r'(?i)( daunorubicine )', ' daunorubicin ')\n",
    "    column = F.regexp_replace(column, r'\\(DNR\\)', '(daunorubicin)')\n",
    "    column = F.regexp_replace(column, r'\\(daunomycin ', '(daunorubicin ')\n",
    "    column = F.regexp_replace(column, r'rubidomicine|rubidomicin', 'daunorubicin')\n",
    "    column = F.regexp_replace(column, r'leukaemomycin C', 'daunorubicin')\n",
    "    column = F.regexp_replace(column, r'(Cerubidine)', '(daunorubicin)')\n",
    "    column = F.regexp_replace(column, r'[Ll]iposomal daunorubicin', 'liposomaldaunorubicin')\n",
    "    column = F.regexp_replace(column, r'(?i)daunoxome', 'daunorubicin')\n",
    "    column = F.regexp_replace(column, r'LDL-daunomycin|LDL:daunomycin', 'low-density-lipoproteins-daunorubicin')\n",
    "    column = F.regexp_replace(column, r'\\(daunomycin\\)', '(daunorubicin)')\n",
    "    column = F.regexp_replace(column, r'\\(daunomycin,', '(daunorubicin,')\n",
    "    column = F.regexp_replace(column, r'LDL-daunomycin', 'low-density-lipoproteins-daunorubicin')\n",
    "    column = F.regexp_replace(column, r'[\\[]?3H[\\]]?[-]?daunomycin', '3H-daunorubicin')\n",
    "    column = F.regexp_replace(column, r' daunomycin\\.', ' daunorubicin.')\n",
    "    column = F.regexp_replace(column, r'13-dihydrodaunomycin', '13-dihydrodaunorubicin')\n",
    "    column = F.regexp_replace(column, r'cys-aconytil-daunomycin', 'cys-aconytil-daunorubicin')\n",
    "    column = F.regexp_replace(column, r'\\[daunomycin\\]', '[daunorubicin]')\n",
    "    column = F.regexp_replace(column, r'porphyrin-daunomycin[a-z]', 'Por-(daunorubicin)')\n",
    "\n",
    "    # azacitidine:\n",
    "    column = F.regexp_replace(column, r'5-AZA|5-azaC|5-AZAC|5-Aza|5-ACR|5-AC', 'azacitidine')\n",
    "    column = F.regexp_replace(column, r\" 5[']?[-]?azac[yi]tidine \", ' azacitidine ')\n",
    "    column = F.regexp_replace(column, r' 5-azac[yi]tidine-', ' azacitidine-')\n",
    "    column = F.regexp_replace(column, r' 5-azac[yi]tidine\\.', ' azacitidine.')\n",
    "    column = F.regexp_replace(column, r' 5-azac[yi]tidine;', ' azacitidine:')\n",
    "    column = F.regexp_replace(column, r' 5-azac[yi]tidine,', ' azacitidine,')\n",
    "    column = F.regexp_replace(column, r' 5-azac[yi]tidine\\)', ' azacitidine)')\n",
    "    column = F.regexp_replace(column, r'(?i)(5-aza-CR)', 'azacitidine')\n",
    "    column = F.regexp_replace(column, r'(?i)( azac[yi]tidine )', ' azacitidine ')\n",
    "    column = F.regexp_replace(column, r'\\(AZA\\)', '(azacitidine)')\n",
    "    column = F.regexp_replace(column, r'(?i)(vidaza)', 'azacitidine')\n",
    "\n",
    "    # gemtuzumab-ozogamicin:\n",
    "    column = F.regexp_replace(column, r\"(?i)(gemtuzumab[- ]?ozogam[yi]cin) \\(GO\\)\", 'gemtuzumab-ozogamicin (gemtuzumab-ozogamicin)')\n",
    "    column = F.regexp_replace(column, r\"(?i)(gemtuzumab[- ]?ozogam[yi]cin) \\(GO,\", 'gemtuzumab-ozogamicin (gemtuzumab-ozogamicin,')\n",
    "    column = F.regexp_replace(column, r\"(?i)(gemtuzumab[- ]?ozogam[yi]cin) \\(GO;\", 'gemtuzumab-ozogamicin (gemtuzumab-ozogamicin;')\n",
    "    column = F.regexp_replace(column, r\"(?i)(gemtuzumab ozogam[yi]cin)\", 'gemtuzumab-ozogamicin')\n",
    "    column = F.regexp_replace(column, r\"CMA-676|FLASI-GO\", 'gemtuzumab-ozogamicin')\n",
    "    column = F.regexp_replace(column, r\"(?i)(my[o]?lotarg)\", 'gemtuzumab-ozogamicin')\n",
    "\n",
    "    # midostaurin:\n",
    "    column = F.regexp_replace(column, r\"(?i)(4'-N-benzoyl staurosporine|4'-N-Benzoylstaurosporine|N-Benzoylstaurosporine)\", 'midostaurin')\n",
    "    column = F.regexp_replace(column, r\"(?i)(rydapt)\", 'midostaurin')\n",
    "    column = F.regexp_replace(column, r\"PKC[ -]?412|CGP[ -]?41251|CAS 120685-11-2\", 'midostaurin')\n",
    "\n",
    "    # CPX-351 (ou vyxeos):\n",
    "    column = F.regexp_replace(column, r\"(?i)(cpx[- ]?351)\", 'vyxeos')\n",
    "    column = F.regexp_replace(column, r\"(?i)(vyxeos liposomal)\", 'vyxeos')\n",
    "    column = F.regexp_replace(column, r\"(?i)(Daunorubicin[ ]?\\/[ ]?cytarabine liposome|liposomaldaunorubicin/cytarabine)\", 'vyxeos')\n",
    "\n",
    "    # ivosidenib:\n",
    "    column = F.regexp_replace(column, r\"(?i)(tibsovo)\", 'ivosidenib')\n",
    "    column = F.regexp_replace(column, r\"(?i)(ag120)\", 'ivosidenib')\n",
    "\n",
    "    # venetoclax:\n",
    "    column = F.regexp_replace(column, r\"(?i)(ABT[ -]?199|GDC[ -]?0199|venclyxto|venclexta)\", 'venetoclax')\n",
    "\n",
    "    # enasidenib:\n",
    "    column = F.regexp_replace(column, r\"(?i)(ag[ -]?221|idhifa|cc[ -]?90007)\", 'enasidenib')\n",
    "\n",
    "    # gilteritinib:\n",
    "    column = F.regexp_replace(column, r\"Xospata|ASP2215\", 'gilteritinib')\n",
    "\n",
    "    # glasdegib:\n",
    "    column = F.regexp_replace(column, r\"\\(DAU\\)\", '(glasdegib)')\n",
    "    column = F.regexp_replace(column, r\"(DAURISMO|PF[ -][0]?4449913|PF[ -]913)\", 'glasdegib')\n",
    "\n",
    "    # arsenic trioxide:\n",
    "    column = F.regexp_replace(column, r\"(?i)(arsenic trioxide)\", 'arsenictrioxide')\n",
    "    column = F.regexp_replace(column, r\"As2[O0]3,\", 'arsenictrioxide,')\n",
    "    column = F.regexp_replace(column, r\"As2[O0]3\\)\", 'arsenictrioxide)')\n",
    "    column = F.regexp_replace(column, r\"As2[O0]3\\.\", 'arsenictrioxide.')\n",
    "    column = F.regexp_replace(column, r\"As2[O0]3$\", 'arsenictrioxide.')\n",
    "    column = F.regexp_replace(column, r\"As2[O0]3 \", 'arsenictrioxide ')\n",
    "    column = F.regexp_replace(column, r\"As2[O0]3\", 'arsenictrioxide')\n",
    "    column = F.regexp_replace(column, r\"As\\(2\\)[O0]\\(3\\) \", 'arsenictrioxide ')\n",
    "    column = F.regexp_replace(column, r\" ATO \", ' arsenictrioxide ')\n",
    "    column = F.regexp_replace(column, r\"\\(ATO\\)\", '(arsenictrioxide)')\n",
    "    column = F.regexp_replace(column, r\"\\(ATO[,;]\", '(arsenictrioxide,')\n",
    "    column = F.regexp_replace(column, r\" ATO-\", ' arsenictrioxide-')\n",
    "    column = F.regexp_replace(column, r\"(?i)(trisenoxt|trisenox)\", 'arsenictrioxide')\n",
    "    column = F.regexp_replace(column, r\"(?i)(arsenic\\(III\\) oxide)\", 'arsenictrioxide')\n",
    "\n",
    "    # cyclophosphamide:\n",
    "    column = F.regexp_replace(column, r\"(?i)(methylerythritol cyclophosphane)\", 'methylerythritolcyclophosphamide')\n",
    "    column = F.regexp_replace(column, r\"(?i)(cyclophosphane)\", 'cyclophosphamide')\n",
    "    column = F.regexp_replace(column, r\"cyclophosphamide-endoxan|Cyklofosfamid|NSC[ -]?26271|Genoxal\", 'cyclophosphamide')\n",
    "    column = F.regexp_replace(column, r\"(?i)(endoxane|endoxan|cyclophosphamidum)\", 'cyclophosphamide')\n",
    "    column = F.regexp_replace(column, r\"(?i)( neosar\\))\", ' cyclophosphamide)')\n",
    "\n",
    "    # dexamethasone:\n",
    "    column = F.regexp_replace(column, r\"(?i)(decadron|dexasone|dexason|Dextenza|Dexycu)\", 'dexamethasone')\n",
    "    column = F.regexp_replace(column, r\"Maxidex|dexasone|Hexadrol|Oradexon|Fortecortin\", 'dexamethasone')\n",
    "    column = F.regexp_replace(column, r\"[Ii]ntratympanic dexamethazone\", 'intratympanicdexamethasone')\n",
    "    column = F.regexp_replace(column, r\"(?i)(de[sx]ametha[sz]one|Desametasone)\", 'dexamethasone')\n",
    "    column = F.regexp_replace(column, r\"dexamethasone \\(DMS\\)\", 'dexamethasone (dexamethasone)')\n",
    "\n",
    "    # idarubicin:\n",
    "    column = F.regexp_replace(column, r\"NSC-256439|Zavedos\", 'idarubicin')\n",
    "    column = F.regexp_replace(column, r\"\\(4-demethoxydauno(mycin|rubicin)\\)\", '(idarubicin)')\n",
    "    column = F.regexp_replace(column, r\"\\(4-demethoxydaunorubicin;\", '(idarubicin;')\n",
    "    column = F.regexp_replace(column, r\" 4-demethoxydauno(mycin|rubicin) \", ' idarubicin ')\n",
    "    column = F.regexp_replace(column, r\" 4-demethoxydauno(mycin|rubicin)\\.\", ' idarubicin.')\n",
    "    column = F.regexp_replace(column, r\" 4-demethoxydauno(mycin|rubicin),\", ' idarubicin,')\n",
    "    column = F.regexp_replace(column, r\"\\[14-14C\\]4-demethoxydaunorubicin HCl\", '[14-14C]idarubicinhcl')\n",
    "    column = F.regexp_replace(column, r\"(?i)(Idarubicine)\", 'idarubicin')\n",
    "    column = F.regexp_replace(column, r\"(?i)(idarubicin hcl)\", 'idarubicin')\n",
    "    column = F.regexp_replace(column, r\"(?i)(4-DMD[N]?R )\", 'idarubicin ')\n",
    "    column = F.regexp_replace(column, r\"(?i)(4-DMD[N]?R[.;])\", 'idarubicin,')\n",
    "    column = F.regexp_replace(column, r\"(?i)(4-DMD[N]?R\\.)\", 'idarubicin.')\n",
    "    column = F.regexp_replace(column, r\"(?i)(4-DMD[N]?R\\))\", 'idarubicin)')\n",
    "    column = F.regexp_replace(column, r\"(?i)(4-DMD[N]?R-)\", 'idarubicin')\n",
    "\n",
    "    # mitoxantrone:\n",
    "    column = F.regexp_replace(column, r\"mitoxantron,cytarabine\", 'mitoxantrone, cytarabine')\n",
    "    column = F.regexp_replace(column, r\"(?i)(mito[zx]ant[h]?rone|novantrone)\", 'mitoxantrone')\n",
    "    column = F.regexp_replace(column, r\"NSC[ -]?301739\", 'mitoxantrone')\n",
    "    column = F.regexp_replace(column, r\"CL[ -]232[,]?315\", 'mitoxantronehydrochloride')\n",
    "    column = F.regexp_replace(column, r\"mitoxantron \", 'mitoxantrone ')\n",
    "    column = F.regexp_replace(column, r\"mitoxantron\\.\", 'mitoxantrone.')\n",
    "    column = F.regexp_replace(column, r\"(?i)(mitoxantrone \\(mit\\))\", 'mitoxantrone (mitoxantrone)')\n",
    "    column = F.regexp_replace(column, r\"(?i)(mitoxantrone \\(mit,)\", 'mitoxantrone (mitoxantrone,')\n",
    "    column = F.regexp_replace(column, r\"(?i)(mitoxantrone) \\((MTX|MIP|MX)\\)\", 'mitoxantrone (mitoxantrone)')\n",
    "    column = F.regexp_replace(column, r\"(?i)(mitoxantrone hydrochloride) \\((MIT|MTO)\\)\", 'mitoxantronehydrochloride (mitoxantronehydrochloride)')\n",
    "    column = F.regexp_replace(column, r\"(?i)(mitoxantrone hydrochloride)\", 'mitoxantronehydrochloride')\n",
    "    column = F.regexp_replace(column, r\"(?i)(1,4-dihydroxy-5,8-bis[ ]?\\(\\([ ]?\\(2-\\[\\(2-hydroxyethyl\\)amino\\]ethyl\\)[ -]?amino\\)\\)-9,10-anthracenedione dihydrochloride)\", 'mitoxantrone')\n",
    "\n",
    "    # pemigatinib:\n",
    "    column = F.regexp_replace(column, r\"PEMAZYRE\", 'pemigatinib')\n",
    "\n",
    "    # prednisone:\n",
    "    column = F.regexp_replace(column, r\"1-dehydrocortisone|[Dd]eltasone|meticorten|NSC[ -]?10023\", 'prednisone')\n",
    "    column = F.regexp_replace(column, r\"(?i)(ultracorten-H|ultracorten H|ultracortene|ultracorten)\", 'prednisone')\n",
    "\n",
    "    # rituximab:\n",
    "    column = F.regexp_replace(column, r\"Rituxan\", 'Rituximab')\n",
    "    column = F.regexp_replace(column, r\"rituxan|\\[RTX-(EU|US)\\]|Truxima|CT-P10\", 'rituximab')\n",
    "    column = F.regexp_replace(column, r\"(?i)(MabThera)\", 'rituximab')\n",
    "    column = F.regexp_replace(column, r\"rituximab/Rituximab\", 'rituximab rituximab')\n",
    "\n",
    "    # thioguanine:\n",
    "    column = F.regexp_replace(column, r\"[Tt]ioguanine|NSC-752\", 'thioguanine')\n",
    "    column = F.regexp_replace(column, r\"(?i)(6-thioguanine| 6 thioguanine)\", 'thioguanine')\n",
    "    column = F.regexp_replace(column, r\"daunorubicin-cytarabine-6 thioguanine\", 'daunorubicin cytarabine thioguanine')\n",
    "    column = F.regexp_replace(column, r\"\\(6TG\\)\", '(thioguanine)')\n",
    "    column = F.regexp_replace(column, r\" 6TG \", ' thioguanine ')\n",
    "    column = F.regexp_replace(column, r\" 6TG\\.\", ' thioguanine.')\n",
    "    column = F.regexp_replace(column, r\" 6TG,\", ' thioguanine,')\n",
    "    column = F.regexp_replace(column, r\"6TG-\", 'thioguanine-')\n",
    "    column = F.regexp_replace(column, r\"2-amino-6-mercaptopurine|6-mercaptoguanine\", 'thioguanine')\n",
    "    column = F.regexp_replace(column, r\"6[ -]?TG[Rr]\", 'thioguanine resistance')\n",
    "    column = F.regexp_replace(column, r\"(?i)(thioguanine \\(tg\\))\", 'thioguanine (thioguanine)')\n",
    "\n",
    "    # vincristine:\n",
    "    column = F.regexp_replace(column, r\"vincrystine|NSC[ -]67574\", 'vincristine')\n",
    "    column = F.regexp_replace(column, r\"(?i)(vincristine sulfate)\", 'vincristinesulfate')\n",
    "    column = F.regexp_replace(column, r\"\\[3H\\][-]?vincristine|3Hvincristine\", '3H-vincristine')\n",
    "    column = F.regexp_replace(column, r\"\\[3H\\][-]?VCR|3HVCR\", '3H-vincristine')\n",
    "    column = F.regexp_replace(column, r\"[Vv]incristine \\(VCR\\)\", 'vincristine (vincristine)')\n",
    "    column = F.regexp_replace(column, r\"CAS 57-22-7\", 'vincristine')\n",
    "    column = F.regexp_replace(column, r\"vincristine,cytarabine\", 'vincristine, cytarabine')\n",
    "\n",
    "    # C-1027:\n",
    "    column = F.regexp_replace(column, r\"(?i)(lidamycin\\(LDM\\))\", 'c-1027 (c-1027)')\n",
    "    column = F.regexp_replace(column, r\"lidamycin\", 'c-1027')\n",
    "\n",
    "    # glyceryl behenate:\n",
    "    column = F.regexp_replace(column, r\"glyceryl behenate\", 'glycerylbehenate')\n",
    "    column = F.regexp_replace(column, r\"[Cc]ompritol 888 ATO\", 'glycerylbehenate')\n",
    "\n",
    "    # decitabine:\n",
    "    column = F.regexp_replace(column, r\"2'-deoxy-(beta-D|beta-d|β-d|β-D)-5-azacytidine\", 'decitabine')\n",
    "    column = F.regexp_replace(column, r\"2'-deoxy-(beta-L|beta-l|β-l|β-L)-5-azacytidine\", 'l-decitabine')\n",
    "\n",
    "    # daunomycinone:\n",
    "    column = F.regexp_replace(column, r'daunomycin aglycone', 'daunomycinone')\n",
    "    column = F.regexp_replace(column, r'13-dihydrodaunomycinone', 'feudomycinonea')\n",
    "\n",
    "    # valrubicin:\n",
    "    column = F.regexp_replace(column, r'AD32|AD 32|AD-32', 'valrubicin')\n",
    "    column = F.regexp_replace(column, r'(?i)(N-Trifluoroacetyladriamycin[ -]14-valerate)', 'valrubicin')\n",
    "\n",
    "    # carmustine:\n",
    "    column = F.regexp_replace(column, r'NSC-409962', 'carmustine')\n",
    "    column = F.regexp_replace(column, r'BCNU|BCNU-NSC', 'carmustine')\n",
    "\n",
    "    # dextromethorphan:\n",
    "    column = F.regexp_replace(column, r'DXM[S]?', 'dextromethorphan')\n",
    "\n",
    "    # docetaxel:\n",
    "    column = F.regexp_replace(column, r'NSC[ -]?628503|RP[ -]?56976', 'docetaxel')\n",
    "\n",
    "    # dactinomycin:\n",
    "    column = F.regexp_replace(column, r'[Aa]ctinomycin[ -]D', 'dactinomycin')\n",
    "\n",
    "    if bert_model == False:\n",
    "        column = F.lower(column)\n",
    "\n",
    "    return column\n",
    "\n",
    "def words_preprocessing(df, column='word', bert_model=False):\n",
    "    fix_typos_dict = {\n",
    "        'citarabine': 'cytarabine',\n",
    "\t    'hdara-c': 'high-dose cytarabine',\n",
    "\t    'no-arac': 'n4-octadecyl-1-beta-d-arabinofuranosylcytosine',\n",
    "\t    'ara-c-ab': 'arac-agarose-bead',\n",
    "\t    'anhydro-ara-fc': \"2,2'-anhydro-1-beta-d-arabinofuranosyl-5-fluorocytosine\",\n",
    "        'mol-ecule': 'molecule',\n",
    "        '‑': '-',\n",
    "        '‒': '-',\n",
    "        '–': '-',\n",
    "        '—': '-',\n",
    "        '¯': '-',\n",
    "        'à': 'a',\n",
    "        'á': 'a',\n",
    "        'â': 'a',\n",
    "        'ã': 'a',\n",
    "        'ä': 'a',\n",
    "        'å': 'a',\n",
    "        'ç': 'c',\n",
    "        'è': 'e',\n",
    "        'é': 'e',\n",
    "        'ê': 'e',\n",
    "        'ë': 'e',\n",
    "        'í': 'i',\n",
    "        'î': 'i',\n",
    "        'ï': 'i',\n",
    "        'ñ': 'n',\n",
    "        'ò': 'o',\n",
    "        'ó': 'o',\n",
    "        'ô': 'o',\n",
    "        'ö': 'o',\n",
    "        '×': 'x',\n",
    "        'ø': 'o',\n",
    "        'ú': 'u',\n",
    "        'ü': 'u',\n",
    "        'č': 'c',\n",
    "        'ğ': 'g',\n",
    "        'ł': 'l',\n",
    "        'ń': 'n',\n",
    "        'ş': 's',\n",
    "        'ŭ': 'u',\n",
    "        'і': 'i',\n",
    "        'ј': 'j',\n",
    "        'а': 'a',\n",
    "        'в': 'b',\n",
    "        'н': 'h',\n",
    "        'о': 'o',\n",
    "        'р': 'p',\n",
    "        'с': 'c',\n",
    "        'т': 't',\n",
    "        'ӧ': 'o',\n",
    "        '⁰': '0',\n",
    "        '⁴': '4',\n",
    "        '⁵': '5',\n",
    "        '⁶': '6',\n",
    "        '⁷': '7',\n",
    "        '⁸': '8',\n",
    "        '⁹': '9',\n",
    "        '₀': '0',\n",
    "        '₁': '1',\n",
    "        '₂': '2',\n",
    "        '₃': '3',\n",
    "        '₅': '5',\n",
    "        '₇': '7',\n",
    "        '₉': '9',\n",
    "    }\n",
    "\n",
    "    units_and_symbols = [\n",
    "        '/μm', '/mol', '°c', '≥', '≤', '<', '>', '±', '%', '/mumol',\n",
    "        'day', 'month', 'year', '·', 'week', 'days',\n",
    "        'weeks', 'years', '/µl', 'μg', 'u/mg',\n",
    "        'mg/m', 'g/m', 'mumol/kg', '/week', '/day', 'm²', '/kg', '®',\n",
    "        'ﬀ', 'ﬃ', 'ﬁ', 'ﬂ', '£', '¥', '©', '«', '¬', '®', '°', '±', '²', '³',\n",
    "        '´', '·', '¹', '»', '½', '¿',\n",
    "         '׳', 'ᇞ​', '‘', '’', '“', '”', '•',  '˂', '˙', '˚', '˜' ,'…', '‰', '′',\n",
    "        '″', '‴', '€',\n",
    "        '™', 'ⅰ', '↑', '→', '↓', '∗', '∙', '∝', '∞', '∼', '≈', '≠', '≤', '≥', '≦', '≫', '⊘',\n",
    "        '⊣', '⊿', '⋅', '═', '■', '▵', '⟶', '⩽', '⩾', '、', '气', '益', '粒', '肾', '补',\n",
    "        '颗', '', '', '', '', '，'\n",
    "    ]\n",
    "\n",
    "    if bert_model:\n",
    "        units_and_symbols_expr = '(%s)' % '|'.join(units_and_symbols[28:])\n",
    "\n",
    "    else:\n",
    "        units_and_symbols_expr = '(%s)' % '|'.join(units_and_symbols)\n",
    "\n",
    "    def __keep_only_compound_numbers():\n",
    "        return F.when(\n",
    "            F.regexp_replace(F.lower(F.col(column)), r'\\d+', '') == F.lit(''),\n",
    "            F.lit('')\n",
    "        ).otherwise(F.lower(F.col(column)))\n",
    "\n",
    "    if bert_model:\n",
    "        return df\\\n",
    "            .replace(fix_typos_dict, subset=column)\\\n",
    "            .withColumn(column, F.regexp_replace(F.col(column), units_and_symbols_expr, ''))\\\n",
    "            .withColumn(column, F.trim(F.col(column)))\\\n",
    "\n",
    "    else:\n",
    "        return df\\\n",
    "            .replace(fix_typos_dict, subset=column)\\\n",
    "            .withColumn(column, F.regexp_replace(F.col(column), units_and_symbols_expr, ''))\\\n",
    "            .withColumn(column, __keep_only_compound_numbers())\\\n",
    "            .withColumn(column, F.trim(F.col(column)))\\\n",
    "            .where(F.length(F.col(column)) > F.lit(1))\\\n",
    "            .where(~F.col(column).isin(nltk.corpus.stopwords.words('english')))"
   ],
   "id": "a1386f9d6db6170e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A parte que mais importa",
   "id": "9df8dc7814b38fb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    # Baixando conjuntos relevantes de palavras.\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "    MATCHED_SYNONYMS_PATH = './data/matched_synonyms/'\n",
    "    CLEAN_PAPERS_PATH = './data/clean_results/'\n",
    "    SYNONYM_ENTITIES = [x.lower() for x in ['Drug', 'Clinical_Drug', 'Pharmacologic_Substance']]\n",
    "\n",
    "    # Cria a sessão do pyspark.\n",
    "    ss()\n",
    "\n",
    "    # janelas de agregação, definem critérios para agupamento de linhas do Spark Dataframe:\n",
    "    w1 = Window.partitionBy(F.col('summary')).orderBy(F.col('filename'))\n",
    "    w2 = Window.partitionBy(F.col('filename'), F.col('id')).orderBy(F.col('pos'))\n",
    "\n",
    "    print('Preprocessing text for Word2Vec models')\n",
    "\n",
    "    print('Replace synonyms: ' + str(True) + '\\n')\n",
    "\n",
    "    #####################################################################\n",
    "    # PASSO 1\n",
    "    # se for desejado substituir os compostos/drogas a partir de dados do PubChem:\n",
    "        # cria a tabela de sinonimos. A primeira coluna contém o sinônimo do composto e a segunda coluna contém o nome (título) do composto ao qual aquele sinônimo se refere.\n",
    "        # a coluna com os nomes dos sinônimos é transformada (sofre processamento), enquanto o título é apenas transformado em letras minúsculas\n",
    "        # o grau da tabela original não é alterado. Ou seja, mantém-se a proporção 1 linha = 1 sinônimo\n",
    "\n",
    "    # se o processamento do texto estiver sendo feito para treinamento de futuros modelos Word2Vec:\n",
    "        # também é realizada a leitura do arquivo de texto que contém as palavras mais comuns do inglês. Esse arquivo é transformado em um DataFrame, removendo-se aquelas palavras selecionadas para o processo de validação\n",
    "        # o DataFrame de palavras em inglês será usado para remover tais palavras do texto, antes do treinamento dos modelos.\n",
    "    #####################################################################\n",
    "\n",
    "    # se for ser realizada a normalização de sinônimos de compostos/drogas, é necessário criar seus Dataframes (incluindo o Dataframe de NER):\n",
    "    synonyms = read_csv_table_files('./pubchem/synonyms/')\n",
    "    synonyms = synonyms\\\n",
    "                .filter(F.col('cid') != \"122172881\")\\\n",
    "                .filter(F.col('cid') != \"11104792\")\n",
    "\n",
    "    titles = read_csv_table_files('./pubchem/titles.csv', sep='|')\n",
    "    titles = titles\\\n",
    "            .filter(F.col('cid') != \"122172881\")\\\n",
    "            .filter(F.col('cid') != \"11104792\")\n",
    "\n",
    "    ner_df = read_csv_table_files('./ner/')\\\n",
    "            .where(F.col('entity').isin(SYNONYM_ENTITIES))\n",
    "\n",
    "    print('ner_df:')\n",
    "    ner_df.show(truncate=False)\n",
    "\n",
    "\n",
    "    # se a normalização de sinônimos for ser realizada para futuro treinamento de modelos Word2vec, o Dataframe de sinônimos deve ser unido (join) ao Dataframe de palavras comuns do inglês,\n",
    "    # pois elas serão removidas do texto:\n",
    "    synonyms = synonyms\\\n",
    "            .withColumn('synonym', F.regexp_replace(F.lower(F.col('synonym')), r'\\s+', ''))\\\n",
    "            .groupby('synonym')\\\n",
    "            .agg(F.min('cid').alias('cid'))\\\n",
    "            .join(titles, 'cid')\\\n",
    "            .withColumn('synonym_title', F.regexp_replace(F.lower(F.col('title')), r'\\s+', ''))\\\n",
    "            .select('synonym', 'synonym_title')\n",
    "\n",
    "    # independentemente de qual o futuro modelo a ser treinado, se houver noralização de sinônimos, o Dataframe de sinônimos é unido com o NER,\n",
    "    # para que haja a normalização apenas de palavras identificadas como drogas/compostos/fármacos:\n",
    "    synonyms = synonyms\\\n",
    "                .filter(F.col('synonym_title') != 'methyl(9r,10s,11s,12r,19r)-11-acetyloxy-12-ethyl-4-[(13s,15r,17s)-17-ethyl-17-hydroxy-13-methoxycarbonyl-1,11-diazatetracyclo[13.3.1.04,12.05,10]nonadeca-4(12),5,7,9-tetraen-13-yl]-8-formyl-10-hydroxy-5-methoxy-8,16-diazapentacyclo[10.6.1.01,9.02,7.016,19]nonadeca-2,4,6,13-tetraene-10-carboxylate')\\\n",
    "                .filter(F.col('synonym_title') != 'methyl(1r,10s,11r,12r,19r)-11-acetyloxy-12-ethyl-4-[(13s,15r,17s)-17-ethyl-17-hydroxy-13-methoxycarbonyl-1,11-diazatetracyclo[13.3.1.04,12.05,10]nonadeca-4(12),5,7,9-tetraen-13-yl]-10-hydroxy-5-methoxy-8-methyl-8,16-diazapentacyclo[10.6.1.01,9.02,7.016,19]nonadeca-2,4,6,13-tetraene-10-carboxylate')\n",
    "\n",
    "    synonyms = synonyms\\\n",
    "                .where(F.col('synonym') != F.col('synonym_title'))\\\n",
    "                .join(ner_df, F.col('synonym') == F.col('token'), 'inner')\\\n",
    "                .drop(*('token', 'entity'))\n",
    "\n",
    "    #####################################################################\n",
    "    # PASSO 2\n",
    "    # cria o DataFrame de artigos limpos/processados. Cada linha dessa tabela equivale a um artigo.\n",
    "    # a tabela tem três colunas: filename, id, summary\n",
    "    #       \"filename\" é o nome do arquivo de onde o artigo foi retirado (results_aggregated), ou seja, é o ano de publicação do artigo.\n",
    "    #       \"id\" é uma coluna serial, apenas para contagem/identificação\n",
    "    #       \"summary\" é o próprio texto (título e/ou prefácio do artigo) limpo/processado.\n",
    "    #####################################################################\n",
    "\n",
    "    cleaned_documents = read_csv_table_files('../bert/results/', sep='|')\n",
    "    print('Abstracts originais:')\n",
    "    cleaned_documents.show(truncate=False)\n",
    "\n",
    "    cleaned_documents = cleaned_documents\\\n",
    "                        .withColumn('summary', summary_column_preprocessing(F.col('summary'), bert_model=False))\\\n",
    "                        .select('id', 'filename', F.posexplode(F.split(F.col('summary'), ' ')).alias('pos', 'word'))\n",
    "\n",
    "    print('Após summary_column_preprocessing:')\n",
    "    cleaned_documents.show(truncate=False)\n",
    "\n",
    "    cleaned_documents = words_preprocessing(cleaned_documents, bert_model=False)\\\n",
    "                        .withColumn('summary', F.collect_list('word').over(w2))\\\n",
    "                        .groupby('id', 'filename')\\\n",
    "                        .agg(\n",
    "                            F.concat_ws(' ', F.max(F.col('summary'))).alias('summary')\n",
    "                        )\n",
    "\n",
    "    print('Após words_preprocessing:')\n",
    "    cleaned_documents.show(truncate=False)\n",
    "\n",
    "    #####################################################################\n",
    "    # PASSO 3\n",
    "    # se o texto estiver sendo processado para modelos BERT, NÃO é realizada a lemmatização dos verbos e advérbios\n",
    "    # caso contrário, é realizada a lemmatização\n",
    "    # em ambos os casos, o texto (summary) é tokenizado nos espaços em branco, formando uma linha do Dataframe para cada token\n",
    "    # esse novo Dataframe - com os tokens - será utilizado para união (join) com o dataframe de sinônimos\n",
    "    #####################################################################\n",
    "\n",
    "    df = cleaned_documents\\\n",
    "        .select(\n",
    "            'id',\n",
    "            'filename',\n",
    "            F.posexplode(F.split(F.col('summary'), ' ')).alias('pos', 'word')\n",
    "        )\n",
    "\n",
    "    print('Após primeiro posexplode:')\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    df = df\\\n",
    "        .withColumnRenamed('word', 'n_word')\n",
    "\n",
    "    df.show(n=60, truncate=False)\n",
    "\n",
    "    df = df\\\n",
    "        .join(synonyms, F.col('synonym') == F.lower(F.col('n_word')), 'left')\\\n",
    "        .distinct()\n",
    "\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    matched_synonyms = df\\\n",
    "        .where(F.col('synonym_title').isNotNull())\\\n",
    "        .select(F.col('synonym'), F.col('synonym_title'))\\\n",
    "        .distinct()\\\n",
    "        .where(F.col('synonym') != F.col('synonym_title'))\n",
    "\n",
    "    df = df\\\n",
    "        .withColumn('word', F.coalesce(F.col('synonym_title'), F.col('n_word')))\\\n",
    "        .drop(*('synonym', 'synonym_title', 'n_word'))\\\n",
    "        .withColumn('summary', F.collect_list('word').over(w2))\\\n",
    "        .groupby('id', 'filename')\\\n",
    "        .agg(\n",
    "            F.concat_ws(' ', F.max(F.col('summary'))).alias('summary')\n",
    "        )\n",
    "\n",
    "    print('Final - após possível normalização de sinônimos:')\n",
    "    df = df.withColumn('id', F.monotonically_increasing_id())\n",
    "    df.show(n=60, truncate=False)\n",
    "\n",
    "    print('Escrevendo csv')\n",
    "    to_csv(df, target_folder=CLEAN_PAPERS_PATH)\n",
    "\n",
    "    to_csv(matched_synonyms, target_folder=MATCHED_SYNONYMS_PATH)\n",
    "\n",
    "    df.printSchema()\n",
    "    print('END!')"
   ],
   "id": "edfa8d9512f32793"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Treinamento do modelo\n",
    "\n",
    "Com os dados limpos e pré-processados, o modelo Word2Vec pode ser treinado."
   ],
   "id": "2a169fc592434be0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, re, sys, shutil, itertools\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import RULE_KEEP, RULE_DEFAULT\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "4761d0caf63aa71e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def list_from_txt(file_path):\n",
    "    '''Creates a list of itens based on a .txt file, each line becomes an item.\n",
    "\n",
    "    Args:\n",
    "      file_path: the path where the .txt file was created.\n",
    "    '''\n",
    "\n",
    "    strings_list = []\n",
    "    with open (file_path, 'rt', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            strings_list.append(line.rstrip('\\n'))\n",
    "    return strings_list\n",
    "\n",
    "def clear_folder(dirpath):\n",
    "    \"\"\" Clears all files from a folder, without deleting the folder.\n",
    "\n",
    "        dirpath: the path of the folder.\n",
    "    \"\"\"\n",
    "\n",
    "    for filename in os.listdir(dirpath):\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        try:\n",
    "            shutil.rmtree(filepath)\n",
    "        except OSError:\n",
    "            os.remove(filepath)\n",
    "\n",
    "def get_target_compounds():\n",
    "    return sorted(['cytarabine', 'daunorubicin', 'azacitidine', 'midostaurin', 'gemtuzumab-ozogamicin', 'vyxeos', 'ivosidenib', 'venetoclax', 'enasidenib', 'gilteritinib', 'glasdegib', 'arsenictrioxide', 'cyclophosphamide', 'dexamethasone', 'idarubicin', 'mitoxantrone', 'pemigatinib', 'prednisone', 'rituximab', 'thioguanine', 'vincristine'])\n",
    "\n",
    "def keep_target_compounds(word, countm, min_count):\n",
    "    if word in get_target_compounds() + ['aml']:\n",
    "        return gensim.utils.RULE_KEEP\n",
    "\n",
    "    else:\n",
    "        return gensim.utils.RULE_DEFAULT"
   ],
   "id": "462f4259826b262e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Starting script')\n",
    "\n",
    "    # CONSTANTS:\n",
    "    MODEL_TYPE = 'ft' # 'w2v' for Word2Vec or 'ft' for FastText\n",
    "    if MODEL_TYPE == 'w2v':\n",
    "        os.makedirs('./models_yoy_combination15/', exist_ok=True)\n",
    "        os.makedirs('./models_yoy_combination2/', exist_ok=True)\n",
    "\n",
    "        parameters_combination = [[100, 0.0025, 10], [200, 0.025, 15]]\n",
    "\n",
    "    else:\n",
    "        os.makedirs('../fasttext/models_yoy_combination16/', exist_ok=True)\n",
    "\n",
    "        parameters_combination = [[300, 0.0025, 5]]\n",
    "\n",
    "    # leitura do arquivo .csv em um DataFrame:\n",
    "    print('Reading DataFrame of papers')\n",
    "    df = pd.read_csv('./pubchem/results_pandas.csv', escapechar='\\\\')\n",
    "\n",
    "    # todos os anos de publicação (sem repetição) presentes no arquivo .csv:\n",
    "    years = sorted(df.filename.unique().tolist())\n",
    "    first_year = years[0]\n",
    "\n",
    "    # intervalos ou \"janelas\" de tempo, a partir do primeiro artigo publicado, até o último. Exemplo:\n",
    "    # supondo que o primeiro artigo coleto foi publicado em 1921, e que, depois deste, mais artigos foram publicados nos anos seguintes, temos:\n",
    "    # [[1921], [1921, 1922], [1921, 1922, 1923], [1921, 1922, 1923, 1924], [1921, 1922, 1923, 1924, 1925], .......]\n",
    "    ranges = [years[:i+1] for i in range(len(years))]\n",
    "    ranges = [x for x in ranges if 1980 in x]\n",
    "    ranges = [x for x in ranges if x[-1] == 1980]\n",
    "\n",
    "    for r in ranges:\n",
    "        print('training model from {} to {}'.format(r[0], r[-1]))\n",
    "        abstracts = df[df.filename.isin(r)]['summary'].to_list()\n",
    "        print('number of abstracts: {}\\n'.format(len(abstracts)))\n",
    "        abstracts = [x.split() for x in abstracts]\n",
    "\n",
    "        # train model\n",
    "        if MODEL_TYPE == 'w2v':            \n",
    "            model_comb15 = Word2Vec(\n",
    "                            # constant parameters:\n",
    "                            sentences=abstracts,\n",
    "                            sorted_vocab=True,\n",
    "                            min_count=5,\n",
    "                            sg=1,\n",
    "                            hs=0,\n",
    "                            iter=15,\n",
    "                            trim_rule=keep_target_compounds,\n",
    "                            # variable parameters:\n",
    "                            size=parameters_combination[1][0],\n",
    "                            alpha=parameters_combination[1][1],\n",
    "                            negative=parameters_combination[1][2]\n",
    "                        )\n",
    "            model_comb15.save('./models_yoy_combination15/model_{}_{}.model'.format(first_year, r[-1]))\n",
    "\n",
    "        else:\n",
    "            model = FastText(\n",
    "                    # constant parameters:\n",
    "                    sentences=abstracts,\n",
    "                    sorted_vocab=True,\n",
    "                    min_count=5,\n",
    "                    sg=1,\n",
    "                    hs=0,\n",
    "                    iter=15,\n",
    "                    trim_rule=keep_target_compounds,\n",
    "                    # variable parameters:\n",
    "                    size=parameters_combination[0][0],\n",
    "                    alpha=parameters_combination[0][1],\n",
    "                    negative=parameters_combination[0][2])\n",
    "            model.save('/fastdata/ac4mvvb/fasttext/models_yoy_combination16/model_{}_{}.model'.format(first_year, r[-1]))\n",
    "\n",
    "    print('END!')"
   ],
   "id": "42ed6f1b8db9e4ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
