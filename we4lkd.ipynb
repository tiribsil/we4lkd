{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui eu vou escrever todo o programa e explicar cada parte :)\n",
    "\n",
    "Primeiro, vou apenas copiar os blocos de código do projeto do Matheus e escrever o que precisa ser alterado.\n",
    "\n",
    "Uma coisa que tenho percebido ao redor do código é a falta de funções auxiliares. Eu pessoalmente não gosto de main longa e cheia, então acho que vou ajustar isso depois."
   ],
   "id": "f9d66732fbf1f68c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Crawler:\n",
    "\n",
    "Aqui serão extraídos os nomes e _abstracts_ dos artigos relacionados aos termos de busca."
   ],
   "id": "dcbaad8d858f6aa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string, os\n",
    "\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp"
   ],
   "id": "a42ce2517510fe13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Essas são funções auxiliares para o funcionamento do _crawler_.",
   "id": "48a9ef7922578bd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def list_from_txt(file_path):\n",
    "    strings_list = []\n",
    "    with open(file_path, 'rt', encoding='utf-8') as list_file:\n",
    "        for line in list_file: strings_list.append(line.rstrip('\\n'))\n",
    "    return strings_list\n",
    "\n",
    "def search(paper_query):\n",
    "    final_query = '{} AND English[Language]'.format(paper_query)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax='999999',\n",
    "                            retmode='xml',\n",
    "                            term=final_query)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def fetch_details(paper_ids):\n",
    "    ids_string = ','.join(paper_ids)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids_string)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def flat(round_list):\n",
    "    flat_list = []\n",
    "    for element in round_list:\n",
    "        if type(element) is list:\n",
    "            for item in element: flat_list.append(item)\n",
    "        else: flat_list.append(element)\n",
    "    return flat_list"
   ],
   "id": "2734a5f1116d69fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Essa ultima função auxiliar é a que insere aos termos de busca sinônimos de compostos para aumentar o escopo da busca.\n",
    "\n",
    "Aqui está _hardcoded_ os IDs dos compostos que terão seus sinônimos inclusos. Esses compostos são tratamentos conhecidos para a AML, e foram levantados manualmente por um especialista. Preciso substituir esses compostos manualmente escolhidos, arrumando alguma forma de obter esses IDs automaticamente para uma doença escolhida. Como? Não sei ainda."
   ],
   "id": "f501a1e62339919a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extend_search():\n",
    "    compound_ids = [\n",
    "        14888, 9444, 62770, 2907, 6253, 122640033, 5743, 90480031, 76970819, 636362, 71657455, 9829523, 51082, 5865,\n",
    "        2723601, 249332, 49846579, 11422859\n",
    "    ]\n",
    "\n",
    "    substance_ids = [\n",
    "        404336834\n",
    "    ]\n",
    "\n",
    "    extended = [pcp.Compound.from_cid(compound_ids[0]).synonyms]\n",
    "\n",
    "    for c in compound_ids[1:]:\n",
    "        extended.extend(pcp.Compound.from_cid(c).synonyms)\n",
    "\n",
    "    for s in substance_ids:\n",
    "        extended.extend(pcp.Substance.from_sid(s).synonyms)\n",
    "\n",
    "    extended = flat(extended)\n",
    "    remove_words = []\n",
    "    for s in extended:\n",
    "        if '[as the base]' in s or '[Poison]' in s or '[ISO]' in s or '[INN]' in s or 'USP/JAN' in s or '(JAN' in s or '[JAN' in s or 'Latin' in s or 'Spanish' in s or '(TN)' in s or '(INN)' in s or 'USAN' in s or 'JP17/USP' in s or 'Czech' in s or 'German' in s or '[CAS]' in s:\n",
    "            remove_words.append(s)\n",
    "\n",
    "    extended = [x for x in extended if x not in remove_words]\n",
    "    extended = [x for x in extended if len(x) >= 3]\n",
    "    extended = list(dict.fromkeys(extended))\n",
    "\n",
    "    useless_strings = list_from_txt('./data/bad_search_strings.txt')\n",
    "    for w in useless_strings:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    remove_words = ['Antibiotic U18496', 'D,L-Cyclophosphamide', 'N,O-propylen-phosphorescence-ester-diamid',\n",
    "                    'UNII-6UXW23996M component CMSMOCZEIVJLDB-CQSZACIVSA-N']\n",
    "    for w in remove_words:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return extended"
   ],
   "id": "f87327d02da77cf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui é o programa principal do _crawler_. Rodar isso aqui demora muito, então não sei ainda como vou testar.\n",
    "\n",
    "Alterações necessárias:\n",
    "\n",
    "   1. Termos de busca não são fixos, preciso de uma função auxiliar que retorna termos de busca para a doença de escolha.\n",
    "\n",
    "   2. Parecido com o 1, mas para os sinônimos.\n",
    "\n",
    "   3. Não acho que esse arquivo de IDs vai continuar existindo."
   ],
   "id": "c494fcc75b0b4af9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    destination_path = './results/'\n",
    "    Path('./data/ids.txt').touch(exist_ok=True)\n",
    "    Path(destination_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Cria uma lista com todos os termos de busca do arquivo. (não vai mais existir esse arquivo)\n",
    "    queries = list_from_txt('./data/queries.txt')\n",
    "    paper_counter = 0\n",
    "\n",
    "    # Insere os sinônimos dos compostos no fim da lista.\n",
    "    synonyms = extend_search()\n",
    "    queries.extend(synonyms)\n",
    "\n",
    "    # Cria uma lista com os IDs de todos os artigos já obtidos e um conjunto de IDs de artigos obtidos.\n",
    "    old_papers = list_from_txt('./data/ids.txt')\n",
    "    ids = set(old_papers)\n",
    "\n",
    "    # Para cada termo de busca...\n",
    "    for query in queries[100:]:\n",
    "        # Normaliza o termo de busca.\n",
    "        query = query.encode('ascii', 'ignore').decode('ascii')\n",
    "        print('searching for {}'.format(query))\n",
    "\n",
    "        # Procura pelo termo de busca, salva os IDs dos artigos encontrados em id_list.\n",
    "        id_list = list(search(query)['IdList'])\n",
    "        # Mantém só os que ainda não estão no conjunto de IDs conhecidos, ou seja, os novos.\n",
    "        id_list = [x for x in id_list if x not in ids]\n",
    "\n",
    "        # Se não achar nada novo, pode ir para o próximo termo de busca.\n",
    "        if not id_list:\n",
    "            print('No new papers found\\n')\n",
    "            continue\n",
    "\n",
    "        print('{} papers found\\n'.format(len(id_list)))\n",
    "        # Insere os novos IDs no conjunto.\n",
    "        ids.update(id_list)\n",
    "\n",
    "        # Pega os detalhes de cada artigo novo.\n",
    "        papers = fetch_details(id_list)\n",
    "\n",
    "        # Cria uma pasta com o nome do termo de busca, formatado para poder ser nome de pasta.\n",
    "        folder_name = query.lower().translate(str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "        Path(destination_path + '{}'.format(folder_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Para cada artigo novo...\n",
    "        for paper in papers['PubmedArticle']:\n",
    "            article_year = ''\n",
    "            article_abstract = ''\n",
    "\n",
    "            # Se não tiver título, vai para o próximo\n",
    "            try:\n",
    "                article_title = paper['MedlineCitation']['Article']['ArticleTitle']\n",
    "                article_title_filename = article_title.lower().translate(\n",
    "                    str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "            except KeyError as e: continue\n",
    "\n",
    "            # Se não tiver prefácio, vai para o próximo.\n",
    "            try:\n",
    "                article_abstract = ' '.join(paper['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "                article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "            except KeyError as e:\n",
    "                if 'Abstract' in e.args: continue\n",
    "                if 'Year' in e.args:\n",
    "                    article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate'][\n",
    "                                       'MedlineDate'][0:4]\n",
    "\n",
    "            # Se não tiver ano válido, vai para o próximo.\n",
    "            if len(article_year) != 4: continue\n",
    "\n",
    "            # O nome do arquivo vai ser \"{ano}_{nome_do_artigo}\"...\n",
    "            filename = '{}_{}'.format(article_year, article_title_filename)\n",
    "            if len(filename) > 150: filename = filename[0:146]\n",
    "\n",
    "            # e vai ser escrito naquela pasta criada antes do loop.\n",
    "            path_name = destination_path + folder_name + '/{}.txt'.format(filename)\n",
    "            path_name = path_name.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "            # Escreve o arquivo.\n",
    "            with open(path_name, \"a\", encoding='utf-8') as file:\n",
    "                file.write(article_title + ' ' + article_abstract)\n",
    "\n",
    "            paper_counter += 1\n",
    "\n",
    "        # Coloca os IDs dos artigos novos no arquivo.\n",
    "        with open('./data/ids.txt', 'a+', encoding='utf-8') as file:\n",
    "            for new_id in id_list: file.write('\\n' + str(new_id))\n",
    "\n",
    "    print('Crawler finished with {} papers collected.'.format(len(old_papers) + paper_counter))"
   ],
   "id": "78526ba501470d3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Agregação dos textos por ano\n",
    "\n",
    "Após pegar todos os prefácios resultantes das buscas, os textos precisam ser agrupados por ano para realizar aquela análise de quão antes o tratamento foi descoberto."
   ],
   "id": "fa8cf281576d1d7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    start_year = 1900\n",
    "    end_year = 2025\n",
    "    source_path = './results/'\n",
    "    destination_path = './results_aggregated/'\n",
    "\n",
    "    # Pega os nomes de todos os arquivos que vieram do crawler.\n",
    "    filenames = list(map(str, Path(source_path).glob('**/*.txt')))\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    # Para cada ano no intervalo...\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Pega os nomes dos arquivos de artigos que são do ano atual.\n",
    "        filenames_current_year = [f for f in filenames if Path(f).stem.startswith(str(year))]\n",
    "\n",
    "        # Se não tiver nenhum desse ano, vai para o próximo.\n",
    "        if not filenames_current_year: continue\n",
    "\n",
    "        print(f'{len(filenames_current_year)} papers from {year}.\\n')\n",
    "\n",
    "        # Pega os prefácios de todos os artigos desse ano.\n",
    "        abstracts = []\n",
    "        for fname in filenames:\n",
    "            with open(fname, encoding='utf-8') as infile:\n",
    "                abstracts.extend(infile.readlines())\n",
    "\n",
    "        # Junta tudo em um só arquivo texto.\n",
    "        output_file = Path(destination_path) / f'results_file_1900_{year}.txt'\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for fname, abstract in zip(filenames, abstracts):\n",
    "                f.write(f\"{Path(fname).stem[5:]}|{abstract}\")"
   ],
   "id": "74bf351f340f69bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Crawler:\n",
    "\n",
    "Aqui serão extraídos os nomes e _abstracts_ dos artigos relacionados aos termos de busca."
   ],
   "id": "6ee4d97548b1fce3"
  },
  {
   "cell_type": "code",
   "id": "0704b06d-2b21-44e5-a224-d68d2dd8deed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:25:05.752226Z",
     "start_time": "2025-03-23T14:25:05.730307Z"
    }
   },
   "source": [
    "import string, os\n",
    "from turtledemo.penrose import start\n",
    "\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp\n",
    "from prompt_toolkit.key_binding.bindings.named_commands import end_of_file"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Essas são funções auxiliares para o funcionamento do _crawler_.",
   "id": "8276a1fe101cfd5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:00:40.192920Z",
     "start_time": "2025-03-23T14:00:40.178647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def list_from_txt(file_path):\n",
    "    strings_list = []\n",
    "    with open(file_path, 'rt', encoding='utf-8') as list_file:\n",
    "        for line in list_file: strings_list.append(line.rstrip('\\n'))\n",
    "    return strings_list\n",
    "\n",
    "def search(paper_query):\n",
    "    final_query = '{} AND English[Language]'.format(paper_query)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax='999999',\n",
    "                            retmode='xml',\n",
    "                            term=final_query)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def fetch_details(paper_ids):\n",
    "    ids_string = ','.join(paper_ids)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids_string)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def flat(round_list):\n",
    "    flat_list = []\n",
    "    for element in round_list:\n",
    "        if type(element) is list:\n",
    "            for item in element: flat_list.append(item)\n",
    "        else: flat_list.append(element)\n",
    "    return flat_list"
   ],
   "id": "377fc3723cfd14b9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Essa ultima função auxiliar é a que insere aos termos de busca sinônimos de compostos para aumentar o escopo da busca.\n",
    "\n",
    "ALTERAÇÕES NECESSÁRIAS: aqui está _hardcoded_ os IDs dos compostos que terão seus sinônimos inclusos. Esses compostos são tratamentos conhecidos para a AML, e foram levantados manualmente por um especialista.\n",
    "\n",
    "SOLUÇÕES?: preciso substituir esses compostos manualmente escolhidos, arrumando alguma forma de obter esses IDs automaticamente para uma doença escolhida. Como? Não sei ainda."
   ],
   "id": "1ea5a37325408cc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:00:45.304022Z",
     "start_time": "2025-03-23T14:00:45.296455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extend_search():\n",
    "    compound_ids = [\n",
    "        14888, 9444, 62770, 2907, 6253, 122640033, 5743, 90480031, 76970819, 636362, 71657455, 9829523, 51082, 5865,\n",
    "        2723601, 249332, 49846579, 11422859\n",
    "    ]\n",
    "\n",
    "    substance_ids = [\n",
    "        404336834\n",
    "    ]\n",
    "\n",
    "    extended = [pcp.Compound.from_cid(compound_ids[0]).synonyms]\n",
    "\n",
    "    for c in compound_ids[1:]:\n",
    "        extended.extend(pcp.Compound.from_cid(c).synonyms)\n",
    "\n",
    "    for s in substance_ids:\n",
    "        extended.extend(pcp.Substance.from_sid(s).synonyms)\n",
    "\n",
    "    extended = flat(extended)\n",
    "    remove_words = []\n",
    "    for s in extended:\n",
    "        if '[as the base]' in s or '[Poison]' in s or '[ISO]' in s or '[INN]' in s or 'USP/JAN' in s or '(JAN' in s or '[JAN' in s or 'Latin' in s or 'Spanish' in s or '(TN)' in s or '(INN)' in s or 'USAN' in s or 'JP17/USP' in s or 'Czech' in s or 'German' in s or '[CAS]' in s:\n",
    "            remove_words.append(s)\n",
    "\n",
    "    extended = [x for x in extended if x not in remove_words]\n",
    "    extended = [x for x in extended if len(x) >= 3]\n",
    "    extended = list(dict.fromkeys(extended))\n",
    "\n",
    "    useless_strings = list_from_txt('./data/bad_search_strings.txt')\n",
    "    for w in useless_strings:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    remove_words = ['Antibiotic U18496', 'D,L-Cyclophosphamide', 'N,O-propylen-phosphorescence-ester-diamid',\n",
    "                    'UNII-6UXW23996M component CMSMOCZEIVJLDB-CQSZACIVSA-N']\n",
    "    for w in remove_words:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return extended"
   ],
   "id": "705c53058a8103fc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui é o programa principal do _crawler_. Rodar isso aqui demora muito, então não sei ainda como vou testar.\n",
    "\n",
    "ALTERAÇÕES NECESSÁRIAS:\n",
    "\n",
    "   1. Termos de busca não são fixos, preciso de uma função auxiliar que retorna termos de busca para a doença de escolha.\n",
    "\n",
    "   2. Parecido com o 1, mas para os sinônimos.\n",
    "\n",
    "   3. Não acho que esse arquivo de IDs vai continuar existindo."
   ],
   "id": "a42bd370c491a485"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:18:01.487269Z",
     "start_time": "2025-03-23T14:02:50.080707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    destination_path = './results/'\n",
    "    Path('./data/ids.txt').touch(exist_ok=True)\n",
    "    Path(destination_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Cria uma lista com todos os termos de busca do arquivo. (não vai mais existir esse arquivo)\n",
    "    queries = list_from_txt('./data/queries.txt')\n",
    "    paper_counter = 0\n",
    "\n",
    "    # Insere os sinônimos dos compostos no fim da lista.\n",
    "    synonyms = extend_search()\n",
    "    queries.extend(synonyms)\n",
    "\n",
    "    # Cria uma lista com os IDs de todos os artigos já obtidos e um conjunto de IDs de artigos obtidos.\n",
    "    old_papers = list_from_txt('./data/ids.txt')\n",
    "    ids = set(old_papers)\n",
    "\n",
    "    # Para cada termo de busca...\n",
    "    for query in queries[100:]:\n",
    "        # Normaliza o termo de busca.\n",
    "        query = query.encode('ascii', 'ignore').decode('ascii')\n",
    "        print('searching for {}'.format(query))\n",
    "\n",
    "        # Procura pelo termo de busca, salva os IDs dos artigos encontrados em id_list.\n",
    "        id_list = list(search(query)['IdList'])\n",
    "        # Mantém só os que ainda não estão no conjunto de IDs conhecidos, ou seja, os novos.\n",
    "        id_list = [x for x in id_list if x not in ids]\n",
    "\n",
    "        # Se não achar nada novo, pode ir para o próximo termo de busca.\n",
    "        if not id_list:\n",
    "            print('No new papers found\\n')\n",
    "            continue\n",
    "\n",
    "        print('{} papers found\\n'.format(len(id_list)))\n",
    "        # Insere os novos IDs no conjunto.\n",
    "        ids.update(id_list)\n",
    "\n",
    "        # Pega os detalhes de cada artigo novo.\n",
    "        papers = fetch_details(id_list)\n",
    "\n",
    "        # Cria uma pasta com o nome do termo de busca, formatado para poder ser nome de pasta.\n",
    "        folder_name = query.lower().translate(str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "        Path(destination_path + '{}'.format(folder_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Para cada artigo novo...\n",
    "        for paper in papers['PubmedArticle']:\n",
    "            article_year = ''\n",
    "            article_abstract = ''\n",
    "\n",
    "            # Se não tiver título, vai para o próximo\n",
    "            try:\n",
    "                article_title = paper['MedlineCitation']['Article']['ArticleTitle']\n",
    "                article_title_filename = article_title.lower().translate(\n",
    "                    str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "            except KeyError as e: continue\n",
    "\n",
    "            # Se não tiver prefácio, vai para o próximo.\n",
    "            try:\n",
    "                article_abstract = ' '.join(paper['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "                article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "            except KeyError as e:\n",
    "                if 'Abstract' in e.args: continue\n",
    "                if 'Year' in e.args:\n",
    "                    article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate'][\n",
    "                                       'MedlineDate'][0:4]\n",
    "\n",
    "            # Se não tiver ano válido, vai para o próximo.\n",
    "            if len(article_year) != 4: continue\n",
    "\n",
    "            # O nome do arquivo vai ser \"{ano}_{nome_do_artigo}\"...\n",
    "            filename = '{}_{}'.format(article_year, article_title_filename)\n",
    "            if len(filename) > 150: filename = filename[0:146]\n",
    "\n",
    "            # e vai ser escrito naquela pasta criada antes do loop.\n",
    "            path_name = destination_path + folder_name + '/{}.txt'.format(filename)\n",
    "            path_name = path_name.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "            # Escreve o arquivo.\n",
    "            with open(path_name, \"a\", encoding='utf-8') as file:\n",
    "                file.write(article_title + ' ' + article_abstract)\n",
    "\n",
    "            paper_counter += 1\n",
    "\n",
    "        # Coloca os IDs dos artigos novos no arquivo.\n",
    "        with open('./data/ids.txt', 'a+', encoding='utf-8') as file:\n",
    "            for new_id in id_list: file.write('\\n' + str(new_id))\n",
    "\n",
    "    print('Crawler finished with {} papers collected.'.format(len(old_papers) + paper_counter))"
   ],
   "id": "a90551a1b1098c7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for HY-10586R\n",
      "No new papers found\n",
      "\n",
      "searching for s1782\n",
      "5 papers found\n",
      "\n",
      "searching for Azacitidine, 5-AzaC;Vidaza;CC-486\n",
      "No new papers found\n",
      "\n",
      "searching for NA02947\n",
      "No new papers found\n",
      "\n",
      "searching for NSC 103-627\n",
      "9999 papers found\n",
      "\n",
      "searching for NCGC00090851-18\n",
      "No new papers found\n",
      "\n",
      "searching for NS00008631\n",
      "No new papers found\n",
      "\n",
      "searching for EN300-118700\n",
      "No new papers found\n",
      "\n",
      "searching for 4-Amino-1-beta-D-ribofuranosyl-D-triazin-2(1H)-one\n",
      "1984 papers found\n",
      "\n",
      "searching for BRD-K03406345-001-21-1\n",
      "No new papers found\n",
      "\n",
      "searching for Z1515383340\n",
      "No new papers found\n",
      "\n",
      "searching for 206-280-2\n",
      "No new papers found\n",
      "\n",
      "searching for 5AE\n",
      "252 papers found\n",
      "\n",
      "searching for InChI=1/C8H12N4O5/c9-7-10-2-12(8(16)11-7)6-5(15)4(14)3(1-13)17-6/h2-6,13-15H,1H2,(H2,9,11,16)/t3-,4-,5-,6-/m1/s1\n",
      "No new papers found\n",
      "\n",
      "searching for 23541-50-6\n",
      "6870 papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE\n",
      "3515 papers found\n",
      "\n",
      "searching for DAUNORUBICIN HCL\n",
      "256 papers found\n",
      "\n",
      "searching for Daunomycin hydrochloride\n",
      "20 papers found\n",
      "\n",
      "searching for Cerubidine\n",
      "3 papers found\n",
      "\n",
      "searching for Rubidomycin hydrochloride\n",
      "5 papers found\n",
      "\n",
      "searching for NDC 0082-4155\n",
      "1279 papers found\n",
      "\n",
      "searching for RP-13057 Hydrochloride\n",
      "9956 papers found\n",
      "\n",
      "searching for EINECS 245-723-4\n",
      "387 papers found\n",
      "\n",
      "searching for Daunoblastin\n",
      "1 papers found\n",
      "\n",
      "searching for Rubomycin\n",
      "No new papers found\n",
      "\n",
      "searching for DTXCID8013382\n",
      "No new papers found\n",
      "\n",
      "searching for DaunoXome\n",
      "24 papers found\n",
      "\n",
      "searching for Daunoblastine\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE (MART.)\n",
      "9 papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [MART.]\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE (USP-RS)\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [USP-RS]\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE (EP MONOGRAPH)\n",
      "1 papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE (USP IMPURITY)\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [EP MONOGRAPH]\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [USP IMPURITY]\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE (USP MONOGRAPH)\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [USP MONOGRAPH]\n",
      "No new papers found\n",
      "\n",
      "searching for Rubilem\n",
      "No new papers found\n",
      "\n",
      "searching for Daunomycin; RP 13057; Rubidomycin\n",
      "No new papers found\n",
      "\n",
      "searching for Daunorubicin.HCl\n",
      "No new papers found\n",
      "\n",
      "searching for Daunomycin, HCl\n",
      "2 papers found\n",
      "\n",
      "searching for Cloridrato de Daunorubicina\n",
      "No new papers found\n",
      "\n",
      "searching for Hydrochloride, Daunorubicin\n",
      "No new papers found\n",
      "\n",
      "searching for Daunorubicini Hydrochloridum\n",
      "No new papers found\n",
      "\n",
      "searching for HB4376\n",
      "No new papers found\n",
      "\n",
      "searching for HY-13062R\n",
      "No new papers found\n",
      "\n",
      "searching for Daunorubicin HCl - Bio-X trade mark\n",
      "No new papers found\n",
      "\n",
      "searching for Daunorubicin hydrochloride (JP18/USP)\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [MI]\n",
      "No new papers found\n",
      "\n",
      "searching for MD11679\n",
      "No new papers found\n",
      "\n",
      "searching for Daunorubicin (hydrochloride) (Standard)\n",
      "44 papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [VANDF]\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [WHO-DD]\n",
      "No new papers found\n",
      "\n",
      "searching for NS00084572\n",
      "No new papers found\n",
      "\n",
      "searching for DAUNORUBICIN HYDROCHLORIDE [ORANGE BOOK]\n",
      "No new papers found\n",
      "\n",
      "searching for (1S,3S)-3-ACETYL-1,2,3,4,6,11-HEXAHYDRO-3,5,12-TRIHYDROXY-10-METHOXY-6,11-DIOXO-1-NAPHTHACENYL 3-AMINO-2,3,6-TRIDEOXY-.ALPHA.-L-LYXO-HEXOPYRANOSIDE HYDROCHLORIDE\n",
      "No new papers found\n",
      "\n",
      "searching for (2S,4S)-2-Acetyl-4-(3-amino-2,3,6-trideoxy-alpha-L-lyxo-hexopyranosyloxy)-2,5,12-trihydroxy-7-methoxy-1,2,3,4-tetrahydrotetracene-6,11-dione monohydrochloride\n",
      "No new papers found\n",
      "\n",
      "searching for (8S,10S)-8-Acetyl-10-[(3-amino-2,3,6-trideoxy-alpha-L-lyxo-hexopyransoyl)oxy]-7,8,9,10-tetrahydro-6,8,11-trihydroxy-1-methoxy-5,12-naphthacenedione hydrochloride\n",
      "No new papers found\n",
      "\n",
      "searching for 245-723-4\n",
      "No new papers found\n",
      "\n",
      "searching for 5,12-NAPHTHACENEDIONE, 8-ACETYL-10-((3-AMINO-2,3,6-TRIDEOXY-.ALPHA.-L-LYXO-HEXOPYRANOSYL))OXY)-7,8,9,10-TETRAHYDRO-6,8,11-TRIHYDROXY-1-METHOXY-, (8S-CIS)-, HYDROCHLORIDE\n",
      "No new papers found\n",
      "\n",
      "searching for 5,12-Naphthacenedione, 8-acetyl-10-((3-amino-2,3,6-trideoxy-alpha-L-lyxo-hexopyranosyl)oxy)-7,8,9,10-tetrahydro-6,8,11-trihydroxy-1-methoxy-, hydrochloride, (8S-cis)-\n",
      "No new papers found\n",
      "\n",
      "searching for L-lyxo-Hexopyranoside, 3beta-acetyl-1,2,3,4,6,11-hexahydro-3,5,12-trihydroxy-10-metldioxo-1a-naphthacenyl 3-amino-2,3,6-trideoxy-,alpha-,hydrochloride\n",
      "No new papers found\n",
      "\n",
      "searching for cyclophosphamide\n",
      "9856 papers found\n",
      "\n",
      "searching for 50-18-0\n",
      "5774 papers found\n",
      "\n",
      "searching for Cytoxan\n",
      "209 papers found\n",
      "\n",
      "searching for Cyclophosphamid\n",
      "No new papers found\n",
      "\n",
      "searching for Cyclophosphane\n",
      "24 papers found\n",
      "\n",
      "searching for Endoxan\n",
      "26 papers found\n",
      "\n",
      "searching for Cytophosphan\n",
      "4 papers found\n",
      "\n",
      "searching for Procytox\n",
      "1 papers found\n",
      "\n",
      "searching for Cyclophosphan\n",
      "No new papers found\n",
      "\n",
      "searching for Mitoxan\n",
      "1 papers found\n",
      "\n",
      "searching for Cyclophosphoramide\n",
      "3 papers found\n",
      "\n",
      "searching for Cyklofosfamid\n",
      "1 papers found\n",
      "\n",
      "searching for Endoxane\n",
      "5 papers found\n",
      "\n",
      "searching for Ciclophosphamide\n",
      "10 papers found\n",
      "\n",
      "searching for ASTA\n",
      "1311 papers found\n",
      "\n",
      "searching for Rcra waste number U058\n",
      "7 papers found\n",
      "\n",
      "searching for CB 4564\n",
      "26 papers found\n",
      "\n",
      "searching for SK 20501\n",
      "No new papers found\n",
      "\n",
      "searching for B 518\n",
      "No new papers found\n",
      "\n",
      "searching for BRN 0011744\n",
      "1717 papers found\n",
      "\n",
      "searching for 1-Bis(2-chloroethyl)amino-1-oxo-2-aza-5-oxaphosphoridin\n",
      "2 papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE LYOPHILIZED\n",
      "21 papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE, ANHYDROUS\n",
      "No new papers found\n",
      "\n",
      "searching for N,N-Bis-(beta-chloraethyl)-N',O-propylen-phosphorsaeure-ester-diamid\n",
      "5951 papers found\n",
      "\n",
      "searching for 2-[Bis(2-chloroethyl)amino]tetrahydro-2H-1,3,2-oxazaphosphorine 2-oxide\n",
      "3 papers found\n",
      "\n",
      "searching for Ciclophosphamide hydrat\n",
      "No new papers found\n",
      "\n",
      "searching for Cytoxan (Lyophilized)\n",
      "No new papers found\n",
      "\n",
      "searching for Cyclofosphamide\n",
      "2 papers found\n",
      "\n",
      "searching for UNII-8N3DW7272P\n",
      "No new papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE (IARC)\n",
      "14 papers found\n",
      "\n",
      "searching for CCRIS 7469\n",
      "No new papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE (USP-RS)\n",
      "No new papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE (EP MONOGRAPH)\n",
      "6 papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE (USP MONOGRAPH)\n",
      "No new papers found\n",
      "\n",
      "searching for RCRA waste no. U058\n",
      "59 papers found\n",
      "\n",
      "searching for Cycloblastine\n",
      "No new papers found\n",
      "\n",
      "searching for Cyclophospham\n",
      "No new papers found\n",
      "\n",
      "searching for Cyclostine\n",
      "No new papers found\n",
      "\n",
      "searching for Fosfaseron\n",
      "No new papers found\n",
      "\n",
      "searching for Syklofosfamid\n",
      "2 papers found\n",
      "\n",
      "searching for Carloxan\n",
      "No new papers found\n",
      "\n",
      "searching for Cicloxal\n",
      "No new papers found\n",
      "\n",
      "searching for Genuxal\n",
      "2 papers found\n",
      "\n",
      "searching for Ledoxina\n",
      "No new papers found\n",
      "\n",
      "searching for Cyclophosphamide?\n",
      "No new papers found\n",
      "\n",
      "searching for 2-(Bis(2-chloroethylamino))-tetrahydro-2H-1,3,2-oxazaphosphorine-2-oxide\n",
      "No new papers found\n",
      "\n",
      "searching for 2-(BIS(2-CHLOROETHYL)AMINO)TETRAHYDRO-2H-1,3,2-OXAZAPHOSPHORINE 2-OXIDE\n",
      "1 papers found\n",
      "\n",
      "searching for CYCLO-cell\n",
      "No new papers found\n",
      "\n",
      "searching for N,N-Bis(beta-cloraethyl) N'-O-propylenphosphorildiamid monohydratum\n",
      "No new papers found\n",
      "\n",
      "searching for bis(2-Chloroethyl)phosphamide cyclic propanolamide ester\n",
      "No new papers found\n",
      "\n",
      "searching for Epitope ID:131782\n",
      "No new papers found\n",
      "\n",
      "searching for C 0768\n",
      "468 papers found\n",
      "\n",
      "searching for DTXCID70364\n",
      "No new papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE [HSDB]\n",
      "No new papers found\n",
      "\n",
      "searching for 2-H-1,3,2-Oxazaphosphorinane\n",
      "No new papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE [WHO-DD]\n",
      "No new papers found\n",
      "\n",
      "searching for L01AA01\n",
      "No new papers found\n",
      "\n",
      "searching for tetrahydro-N,N-bis(2-chloroethyl)-2H-1,3,2-oxazaphosphorin-2-amine 2-oxide\n",
      "No new papers found\n",
      "\n",
      "searching for CYCLOPHOSPHAMIDE ANHYDROUS [MI]\n",
      "No new papers found\n",
      "\n",
      "searching for N,O-propylen-phosphorsaeure-ester-diamid\n",
      "9914 papers found\n",
      "\n",
      "searching for N,O-propylene-phosphoric acid ester diamide\n",
      "34 papers found\n",
      "\n",
      "searching for WR-138719\n",
      "No new papers found\n",
      "\n",
      "searching for C2236\n",
      "No new papers found\n",
      "\n",
      "searching for NS00000339\n",
      "No new papers found\n",
      "\n",
      "searching for A12571\n",
      "No new papers found\n",
      "\n",
      "searching for BRD-A09722536-002-17-2\n",
      "No new papers found\n",
      "\n",
      "searching for BRD-A09722536-002-18-0\n",
      "No new papers found\n",
      "\n",
      "searching for BRD-A09722536-002-19-8\n",
      "No new papers found\n",
      "\n",
      "searching for Z276509078\n",
      "No new papers found\n",
      "\n",
      "searching for 2-(bis(2-chloroethyl)amino)-1,3,2-oxazaphosphinane2-oxide\n",
      "89 papers found\n",
      "\n",
      "searching for Bis(2-chloroethyl)phosphoramide-cyclic propanolamide ester\n",
      "No new papers found\n",
      "\n",
      "searching for 1-BIS(2-CHLOROETHYL)AMINO-1-OXO-2-AZA-5-OXAPHOSPHORIDINE\n",
      "No new papers found\n",
      "\n",
      "searching for 2-[bis(2-chloroethyl)amino]-1,3,2lambda5-oxazaphosphinan-2-one\n",
      "No new papers found\n",
      "\n",
      "searching for N,N-DI(2-CHLOROETHYL)-N,O-PROPYLENE PHOSPHORIC ACID ESTER DIAMIDE\n",
      "No new papers found\n",
      "\n",
      "searching for (+/-)-2-(BIS(2-CHLOROETHYL)AMINO)TETRAHYDRO-2H-1,3,2-OXAZAPHOSPHORINE 2-OXIDE\n",
      "No new papers found\n",
      "\n",
      "searching for 2-(DI(2-CHLOROETHYLAMINO))-1-OXA-3-AZA-2-PHOSPHACYCLOHEXANE 2-OXIDE\n",
      "No new papers found\n",
      "\n",
      "searching for 2-Bis(2-chloroethyl)amino)tetrahydro-2H-1,3,2-oxazaphosphorin-2-amine 2-oxide\n",
      "No new papers found\n",
      "\n",
      "searching for 2H-1,3,2-oxazaphosphorin-2-amine, N,N-is(2-chloroethyl)tetrahydro-,2-oxide\n",
      "No new papers found\n",
      "\n",
      "searching for N,N-bis(2-chloroethyl)-2-oxo-1-oxa-3-aza-2$l^{5}-phosphacyclohexan-2-amine\n",
      "No new papers found\n",
      "\n",
      "searching for N,N-BIS(2-CHLOROETHYL)-N'-(3-HYDROXYPROPYL)PHOSPHORODIAMIDIC ACID INTRAMOL.\n",
      "No new papers found\n",
      "\n",
      "searching for N,N-BIS(BETA-CHLOROETHYL)N',O-TRIMETHYLENEPHOSPHORIC ACID ESTER DIAMIDE\n",
      "No new papers found\n",
      "\n",
      "searching for 200-015-4\n",
      "No new papers found\n",
      "\n",
      "searching for InChI=1/C7H15Cl2N2O2P/c8-2-5-11(6-3-9)14(12)10-4-1-7-13-14/h1-7H2,(H,10,12)\n",
      "No new papers found\n",
      "\n",
      "searching for cytarabine\n",
      "8431 papers found\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 39\u001B[39m\n\u001B[32m     36\u001B[39m ids.update(id_list)\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# Pega os detalhes de cada artigo novo.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m papers = \u001B[43mfetch_details\u001B[49m\u001B[43m(\u001B[49m\u001B[43mid_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# Cria uma pasta com o nome do termo de busca, formatado para poder ser nome de pasta.\u001B[39;00m\n\u001B[32m     42\u001B[39m folder_name = query.lower().translate(\u001B[38;5;28mstr\u001B[39m.maketrans(\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m, string.punctuation)).replace(\u001B[33m'\u001B[39m\u001B[33m \u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33m_\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 25\u001B[39m, in \u001B[36mfetch_details\u001B[39m\u001B[34m(paper_ids)\u001B[39m\n\u001B[32m     21\u001B[39m Entrez.email = \u001B[33m'\u001B[39m\u001B[33mtirs@estudante.ufscar.br\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     22\u001B[39m handle = Entrez.efetch(db=\u001B[33m'\u001B[39m\u001B[33mpubmed\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     23\u001B[39m                        retmode=\u001B[33m'\u001B[39m\u001B[33mxml\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     24\u001B[39m                        \u001B[38;5;28mid\u001B[39m=ids_string)\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m found = \u001B[43mEntrez\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m handle.close()\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m found\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\UFSCar\\IC\\we4lkd\\.venv\\Lib\\site-packages\\Bio\\Entrez\\__init__.py:529\u001B[39m, in \u001B[36mread\u001B[39m\u001B[34m(source, validate, escape, ignore_errors)\u001B[39m\n\u001B[32m    526\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mParser\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataHandler\n\u001B[32m    528\u001B[39m handler = DataHandler(validate, escape, ignore_errors)\n\u001B[32m--> \u001B[39m\u001B[32m529\u001B[39m record = \u001B[43mhandler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    530\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m record\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\UFSCar\\IC\\we4lkd\\.venv\\Lib\\site-packages\\Bio\\Entrez\\Parser.py:405\u001B[39m, in \u001B[36mDataHandler.read\u001B[39m\u001B[34m(self, source)\u001B[39m\n\u001B[32m    403\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mfile should be opened in binary mode\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    404\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m405\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m.\u001B[49m\u001B[43mParseFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    406\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m expat.ExpatError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    407\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.parser.StartElementHandler:\n\u001B[32m    408\u001B[39m         \u001B[38;5;66;03m# We saw the initial <!xml declaration, so we can be sure that\u001B[39;00m\n\u001B[32m    409\u001B[39m         \u001B[38;5;66;03m# we are parsing XML data. Most likely, the XML file is\u001B[39;00m\n\u001B[32m    410\u001B[39m         \u001B[38;5;66;03m# corrupted.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:473\u001B[39m, in \u001B[36mHTTPResponse.read\u001B[39m\u001B[34m(self, amt)\u001B[39m\n\u001B[32m    470\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    472\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.chunked:\n\u001B[32m--> \u001B[39m\u001B[32m473\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_read_chunked\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    475\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt >= \u001B[32m0\u001B[39m:\n\u001B[32m    476\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt > \u001B[38;5;28mself\u001B[39m.length:\n\u001B[32m    477\u001B[39m         \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:597\u001B[39m, in \u001B[36mHTTPResponse._read_chunked\u001B[39m\u001B[34m(self, amt)\u001B[39m\n\u001B[32m    595\u001B[39m value = []\n\u001B[32m    596\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m597\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m (chunk_left := \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_chunk_left\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    598\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt <= chunk_left:\n\u001B[32m    599\u001B[39m             value.append(\u001B[38;5;28mself\u001B[39m._safe_read(amt))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:579\u001B[39m, in \u001B[36mHTTPResponse._get_chunk_left\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    577\u001B[39m     \u001B[38;5;28mself\u001B[39m._safe_read(\u001B[32m2\u001B[39m)  \u001B[38;5;66;03m# toss the CRLF at the end of the chunk\u001B[39;00m\n\u001B[32m    578\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m579\u001B[39m     chunk_left = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_read_next_chunk_size\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    580\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[32m    581\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m IncompleteRead(\u001B[33mb\u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:539\u001B[39m, in \u001B[36mHTTPResponse._read_next_chunk_size\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_read_next_chunk_size\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    538\u001B[39m     \u001B[38;5;66;03m# Read the next chunk size from the file\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m539\u001B[39m     line = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_MAXLINE\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    540\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) > _MAXLINE:\n\u001B[32m    541\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[33m\"\u001B[39m\u001B[33mchunk size\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    717\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mcannot read from timed out object\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    718\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m719\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    720\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    721\u001B[39m     \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1304\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1300\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1301\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1302\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1303\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1304\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1305\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1306\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1138\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1136\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1137\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1138\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1139\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1140\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Agregação dos textos por ano\n",
    "\n",
    "Após pegar todos os prefácios resultantes das buscas, os textos precisam ser agrupados por ano para realizar aquela análise de quão antes o tratamento foi descoberto."
   ],
   "id": "5099efda3cae7e27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:04:16.697868Z",
     "start_time": "2025-03-23T14:57:16.201889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    start_year = 1900\n",
    "    end_year = 2025\n",
    "    source_path = './results/'\n",
    "    destination_path = './results_aggregated/'\n",
    "\n",
    "    # Pega os nomes de todos os arquivos que vieram do crawler.\n",
    "    filenames = list(map(str, Path(source_path).glob('**/*.txt')))\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    # Para cada ano no intervalo...\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Pega os nomes dos arquivos de artigos que são do ano atual.\n",
    "        filenames_current_year = [f for f in filenames if Path(f).stem.startswith(str(year))]\n",
    "\n",
    "        # Se não tiver nenhum desse ano, vai para o próximo.\n",
    "        if not filenames_current_year:\n",
    "            continue\n",
    "\n",
    "        print(f'{len(filenames_current_year)} papers from {year}.\\n')\n",
    "\n",
    "        # Pega os prefácios de todos os artigos desse ano.\n",
    "        abstracts = []\n",
    "        for fname in filenames:\n",
    "            with open(fname, encoding='utf-8') as infile:\n",
    "                abstracts.extend(infile.readlines())\n",
    "\n",
    "        # Junta tudo em um só arquivo texto.\n",
    "        output_file = Path(destination_path) / f'results_file_1900_{year}.txt'\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for fname, abstract in zip(filenames, abstracts):\n",
    "                f.write(f\"{Path(fname).stem[5:]}|{abstract}\")"
   ],
   "id": "a678a4e378250ac9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 papers from 1915.\n",
      "\n",
      "1 papers from 1919.\n",
      "\n",
      "1 papers from 1921.\n",
      "\n",
      "1 papers from 1941.\n",
      "\n",
      "2 papers from 1943.\n",
      "\n",
      "1 papers from 1945.\n",
      "\n",
      "1 papers from 1951.\n",
      "\n",
      "4 papers from 1957.\n",
      "\n",
      "1 papers from 1958.\n",
      "\n",
      "1 papers from 1959.\n",
      "\n",
      "3 papers from 1961.\n",
      "\n",
      "1 papers from 1962.\n",
      "\n",
      "4 papers from 1963.\n",
      "\n",
      "3 papers from 1964.\n",
      "\n",
      "5 papers from 1965.\n",
      "\n",
      "6 papers from 1966.\n",
      "\n",
      "11 papers from 1967.\n",
      "\n",
      "11 papers from 1968.\n",
      "\n",
      "20 papers from 1969.\n",
      "\n",
      "34 papers from 1970.\n",
      "\n",
      "35 papers from 1971.\n",
      "\n",
      "42 papers from 1972.\n",
      "\n",
      "28 papers from 1973.\n",
      "\n",
      "34 papers from 1974.\n",
      "\n",
      "465 papers from 1975.\n",
      "\n",
      "441 papers from 1976.\n",
      "\n",
      "411 papers from 1977.\n",
      "\n",
      "440 papers from 1978.\n",
      "\n",
      "433 papers from 1979.\n",
      "\n",
      "474 papers from 1980.\n",
      "\n",
      "451 papers from 1981.\n",
      "\n",
      "488 papers from 1982.\n",
      "\n",
      "556 papers from 1983.\n",
      "\n",
      "607 papers from 1984.\n",
      "\n",
      "625 papers from 1985.\n",
      "\n",
      "661 papers from 1986.\n",
      "\n",
      "653 papers from 1987.\n",
      "\n",
      "637 papers from 1988.\n",
      "\n",
      "656 papers from 1989.\n",
      "\n",
      "754 papers from 1990.\n",
      "\n",
      "691 papers from 1991.\n",
      "\n",
      "674 papers from 1992.\n",
      "\n",
      "684 papers from 1993.\n",
      "\n",
      "694 papers from 1994.\n",
      "\n",
      "651 papers from 1995.\n",
      "\n",
      "737 papers from 1996.\n",
      "\n",
      "678 papers from 1997.\n",
      "\n",
      "676 papers from 1998.\n",
      "\n",
      "687 papers from 1999.\n",
      "\n",
      "795 papers from 2000.\n",
      "\n",
      "900 papers from 2001.\n",
      "\n",
      "955 papers from 2002.\n",
      "\n",
      "877 papers from 2003.\n",
      "\n",
      "930 papers from 2004.\n",
      "\n",
      "974 papers from 2005.\n",
      "\n",
      "1032 papers from 2006.\n",
      "\n",
      "1101 papers from 2007.\n",
      "\n",
      "1213 papers from 2008.\n",
      "\n",
      "1324 papers from 2009.\n",
      "\n",
      "1468 papers from 2010.\n",
      "\n",
      "1489 papers from 2011.\n",
      "\n",
      "1559 papers from 2012.\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(output_file, \u001B[33m'\u001B[39m\u001B[33mw\u001B[39m\u001B[33m'\u001B[39m, encoding=\u001B[33m'\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m     31\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m fname, abstract \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(filenames, abstracts):\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m         \u001B[43mf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstem\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m|\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mabstract\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mOSError\u001B[39m: [Errno 28] No space left on device"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
