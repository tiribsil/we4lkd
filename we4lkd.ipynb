{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui eu vou escrever todo o programa e explicar cada parte :)\n",
    "\n",
    "Primeiro, vou apenas copiar os blocos de código do projeto do Matheus e escrever o que precisa ser alterado.\n",
    "\n",
    "Uma coisa que tenho percebido ao redor do código é a falta de funções auxiliares. Eu pessoalmente não gosto de main longa e cheia, então acho que vou ajustar isso depois."
   ],
   "id": "f9d66732fbf1f68c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Crawler:\n",
    "\n",
    "Aqui serão extraídos os nomes e _abstracts_ dos artigos relacionados aos termos de busca."
   ],
   "id": "dcbaad8d858f6aa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import string, os\n",
    "\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp"
   ],
   "id": "a42ce2517510fe13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Essas são funções auxiliares para o funcionamento do _crawler_.",
   "id": "48a9ef7922578bd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def list_from_txt(file_path):\n",
    "    strings_list = []\n",
    "    with open(file_path, 'rt', encoding='utf-8') as list_file:\n",
    "        for line in list_file: strings_list.append(line.rstrip('\\n'))\n",
    "    return strings_list\n",
    "\n",
    "def search(paper_query):\n",
    "    final_query = '{} AND English[Language]'.format(paper_query)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax='999999',\n",
    "                            retmode='xml',\n",
    "                            term=final_query)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def fetch_details(paper_ids):\n",
    "    ids_string = ','.join(paper_ids)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids_string)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def flat(round_list):\n",
    "    flat_list = []\n",
    "    for element in round_list:\n",
    "        if type(element) is list:\n",
    "            for item in element: flat_list.append(item)\n",
    "        else: flat_list.append(element)\n",
    "    return flat_list"
   ],
   "id": "2734a5f1116d69fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Essa ultima função auxiliar é a que insere aos termos de busca sinônimos de compostos para aumentar o escopo da busca.\n",
    "\n",
    "Aqui está _hardcoded_ os IDs dos compostos que terão seus sinônimos inclusos. Esses compostos são tratamentos conhecidos para a AML, e foram levantados manualmente por um especialista. Preciso substituir esses compostos manualmente escolhidos, arrumando alguma forma de obter esses IDs automaticamente para uma doença escolhida. Como? Não sei ainda."
   ],
   "id": "f501a1e62339919a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extend_search():\n",
    "    compound_ids = [\n",
    "        14888, 9444, 62770, 2907, 6253, 122640033, 5743, 90480031, 76970819, 636362, 71657455, 9829523, 51082, 5865,\n",
    "        2723601, 249332, 49846579, 11422859\n",
    "    ]\n",
    "\n",
    "    substance_ids = [\n",
    "        404336834\n",
    "    ]\n",
    "\n",
    "    extended = [pcp.Compound.from_cid(compound_ids[0]).synonyms]\n",
    "\n",
    "    for c in compound_ids[1:]:\n",
    "        extended.extend(pcp.Compound.from_cid(c).synonyms)\n",
    "\n",
    "    for s in substance_ids:\n",
    "        extended.extend(pcp.Substance.from_sid(s).synonyms)\n",
    "\n",
    "    extended = flat(extended)\n",
    "    remove_words = []\n",
    "    for s in extended:\n",
    "        if '[as the base]' in s or '[Poison]' in s or '[ISO]' in s or '[INN]' in s or 'USP/JAN' in s or '(JAN' in s or '[JAN' in s or 'Latin' in s or 'Spanish' in s or '(TN)' in s or '(INN)' in s or 'USAN' in s or 'JP17/USP' in s or 'Czech' in s or 'German' in s or '[CAS]' in s:\n",
    "            remove_words.append(s)\n",
    "\n",
    "    extended = [x for x in extended if x not in remove_words]\n",
    "    extended = [x for x in extended if len(x) >= 3]\n",
    "    extended = list(dict.fromkeys(extended))\n",
    "\n",
    "    useless_strings = list_from_txt('./data/bad_search_strings.txt')\n",
    "    for w in useless_strings:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    remove_words = ['Antibiotic U18496', 'D,L-Cyclophosphamide', 'N,O-propylen-phosphorescence-ester-diamid',\n",
    "                    'UNII-6UXW23996M component CMSMOCZEIVJLDB-CQSZACIVSA-N']\n",
    "    for w in remove_words:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return extended"
   ],
   "id": "f87327d02da77cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui é o programa principal do _crawler_. Rodar isso aqui demora muito, então não sei ainda como vou testar.\n",
    "\n",
    "Alterações necessárias:\n",
    "\n",
    "   1. Termos de busca não são fixos, preciso de uma função auxiliar que retorna termos de busca para a doença de escolha.\n",
    "\n",
    "   2. Parecido com o 1, mas para os sinônimos.\n",
    "\n",
    "   3. Não acho que esse arquivo de IDs vai continuar existindo."
   ],
   "id": "c494fcc75b0b4af9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    destination_path = './results/'\n",
    "    Path('./data/ids.txt').touch(exist_ok=True)\n",
    "    Path(destination_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Cria uma lista com todos os termos de busca do arquivo. (não vai mais existir esse arquivo)\n",
    "    queries = list_from_txt('./data/queries.txt')\n",
    "    paper_counter = 0\n",
    "\n",
    "    # Insere os sinônimos dos compostos no fim da lista.\n",
    "    synonyms = extend_search()\n",
    "    queries.extend(synonyms)\n",
    "\n",
    "    # Cria uma lista com os IDs de todos os artigos já obtidos e um conjunto de IDs de artigos obtidos.\n",
    "    old_papers = list_from_txt('./data/ids.txt')\n",
    "    ids = set(old_papers)\n",
    "\n",
    "    # Para cada termo de busca...\n",
    "    for query in queries[100:]:\n",
    "        # Normaliza o termo de busca.\n",
    "        query = query.encode('ascii', 'ignore').decode('ascii')\n",
    "        print('searching for {}'.format(query))\n",
    "\n",
    "        # Procura pelo termo de busca, salva os IDs dos artigos encontrados em id_list.\n",
    "        id_list = list(search(query)['IdList'])\n",
    "        # Mantém só os que ainda não estão no conjunto de IDs conhecidos, ou seja, os novos.\n",
    "        id_list = [x for x in id_list if x not in ids]\n",
    "\n",
    "        # Se não achar nada novo, pode ir para o próximo termo de busca.\n",
    "        if not id_list:\n",
    "            print('No new papers found\\n')\n",
    "            continue\n",
    "\n",
    "        print('{} papers found\\n'.format(len(id_list)))\n",
    "        # Insere os novos IDs no conjunto.\n",
    "        ids.update(id_list)\n",
    "\n",
    "        # Pega os detalhes de cada artigo novo.\n",
    "        papers = fetch_details(id_list)\n",
    "\n",
    "        # Cria uma pasta com o nome do termo de busca, formatado para poder ser nome de pasta.\n",
    "        folder_name = query.lower().translate(str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "        Path(destination_path + '{}'.format(folder_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Para cada artigo novo...\n",
    "        for paper in papers['PubmedArticle']:\n",
    "            article_year = ''\n",
    "            article_abstract = ''\n",
    "\n",
    "            # Se não tiver título, vai para o próximo\n",
    "            try:\n",
    "                article_title = paper['MedlineCitation']['Article']['ArticleTitle']\n",
    "                article_title_filename = article_title.lower().translate(\n",
    "                    str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "            except KeyError as e: continue\n",
    "\n",
    "            # Se não tiver prefácio, vai para o próximo.\n",
    "            try:\n",
    "                article_abstract = ' '.join(paper['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "                article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "            except KeyError as e:\n",
    "                if 'Abstract' in e.args: continue\n",
    "                if 'Year' in e.args:\n",
    "                    article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate'][\n",
    "                                       'MedlineDate'][0:4]\n",
    "\n",
    "            # Se não tiver ano válido, vai para o próximo.\n",
    "            if len(article_year) != 4: continue\n",
    "\n",
    "            # O nome do arquivo vai ser \"{ano}_{nome_do_artigo}\"...\n",
    "            filename = '{}_{}'.format(article_year, article_title_filename)\n",
    "            if len(filename) > 150: filename = filename[0:146]\n",
    "\n",
    "            # e vai ser escrito naquela pasta criada antes do loop.\n",
    "            path_name = destination_path + folder_name + '/{}.txt'.format(filename)\n",
    "            path_name = path_name.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "            # Escreve o arquivo.\n",
    "            with open(path_name, \"a\", encoding='utf-8') as file:\n",
    "                file.write(article_title + ' ' + article_abstract)\n",
    "\n",
    "            paper_counter += 1\n",
    "\n",
    "        # Coloca os IDs dos artigos novos no arquivo.\n",
    "        with open('./data/ids.txt', 'a+', encoding='utf-8') as file:\n",
    "            for new_id in id_list: file.write('\\n' + str(new_id))\n",
    "\n",
    "    print('Crawler finished with {} papers collected.'.format(len(old_papers) + paper_counter))"
   ],
   "id": "78526ba501470d3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Agregação dos textos por ano\n",
    "\n",
    "Após pegar todos os prefácios resultantes das buscas, os textos precisam ser agrupados por ano para realizar aquela análise de quão antes o tratamento foi descoberto."
   ],
   "id": "fa8cf281576d1d7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    start_year = 1900\n",
    "    end_year = 2025\n",
    "    source_path = './results/'\n",
    "    destination_path = './results_aggregated/'\n",
    "\n",
    "    # Pega os nomes de todos os arquivos que vieram do crawler.\n",
    "    filenames = list(map(str, Path(source_path).glob('**/*.txt')))\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    # Para cada ano no intervalo...\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Pega os nomes dos arquivos de artigos que são do ano atual.\n",
    "        filenames_current_year = [f for f in filenames if Path(f).stem.startswith(str(year))]\n",
    "\n",
    "        # Se não tiver nenhum desse ano, vai para o próximo.\n",
    "        if not filenames_current_year: continue\n",
    "\n",
    "        print(f'{len(filenames_current_year)} papers from {year}.\\n')\n",
    "\n",
    "        # Pega os prefácios de todos os artigos desse ano.\n",
    "        abstracts = []\n",
    "        for fname in filenames:\n",
    "            with open(fname, encoding='utf-8') as infile:\n",
    "                abstracts.extend(infile.readlines())\n",
    "\n",
    "        # Junta tudo em um só arquivo texto.\n",
    "        output_file = Path(destination_path) / f'results_file_1900_{year}.txt'\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for fname, abstract in zip(filenames, abstracts):\n",
    "                f.write(f\"{Path(fname).stem[5:]}|{abstract}\")"
   ],
   "id": "74bf351f340f69bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Crawler:\n",
    "\n",
    "Aqui serão extraídos os nomes e _abstracts_ dos artigos relacionados aos termos de busca."
   ],
   "id": "6ee4d97548b1fce3"
  },
  {
   "cell_type": "code",
   "id": "0704b06d-2b21-44e5-a224-d68d2dd8deed",
   "metadata": {},
   "source": [
    "import string, os\n",
    "\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "import pubchempy as pcp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Essas são funções auxiliares para o funcionamento do _crawler_.",
   "id": "8276a1fe101cfd5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def list_from_txt(file_path):\n",
    "    strings_list = []\n",
    "    with open(file_path, 'rt', encoding='utf-8') as list_file:\n",
    "        for line in list_file: strings_list.append(line.rstrip('\\n'))\n",
    "    return strings_list\n",
    "\n",
    "def search(paper_query):\n",
    "    final_query = '{} AND English[Language]'.format(paper_query)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            sort='relevance',\n",
    "                            retmax='999999',\n",
    "                            retmode='xml',\n",
    "                            term=final_query)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def fetch_details(paper_ids):\n",
    "    ids_string = ','.join(paper_ids)\n",
    "    Entrez.email = 'tirs@estudante.ufscar.br'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids_string)\n",
    "    found = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return found\n",
    "\n",
    "def flat(round_list):\n",
    "    flat_list = []\n",
    "    for element in round_list:\n",
    "        if type(element) is list:\n",
    "            for item in element: flat_list.append(item)\n",
    "        else: flat_list.append(element)\n",
    "    return flat_list"
   ],
   "id": "377fc3723cfd14b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Essa ultima função auxiliar é a que insere aos termos de busca sinônimos de compostos para aumentar o escopo da busca.\n",
    "\n",
    "ALTERAÇÕES NECESSÁRIAS: aqui está _hardcoded_ os IDs dos compostos que terão seus sinônimos inclusos. Esses compostos são tratamentos conhecidos para a AML, e foram levantados manualmente por um especialista.\n",
    "\n",
    "SOLUÇÕES?: preciso substituir esses compostos manualmente escolhidos, arrumando alguma forma de obter esses IDs automaticamente para uma doença escolhida. Como? Não sei ainda."
   ],
   "id": "1ea5a37325408cc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extend_search():\n",
    "    compound_ids = [\n",
    "        14888, 9444, 62770, 2907, 6253, 122640033, 5743, 90480031, 76970819, 636362, 71657455, 9829523, 51082, 5865,\n",
    "        2723601, 249332, 49846579, 11422859\n",
    "    ]\n",
    "\n",
    "    substance_ids = [\n",
    "        404336834\n",
    "    ]\n",
    "\n",
    "    extended = [pcp.Compound.from_cid(compound_ids[0]).synonyms]\n",
    "\n",
    "    for c in compound_ids[1:]:\n",
    "        extended.extend(pcp.Compound.from_cid(c).synonyms)\n",
    "\n",
    "    for s in substance_ids:\n",
    "        extended.extend(pcp.Substance.from_sid(s).synonyms)\n",
    "\n",
    "    extended = flat(extended)\n",
    "    remove_words = []\n",
    "    for s in extended:\n",
    "        if '[as the base]' in s or '[Poison]' in s or '[ISO]' in s or '[INN]' in s or 'USP/JAN' in s or '(JAN' in s or '[JAN' in s or 'Latin' in s or 'Spanish' in s or '(TN)' in s or '(INN)' in s or 'USAN' in s or 'JP17/USP' in s or 'Czech' in s or 'German' in s or '[CAS]' in s:\n",
    "            remove_words.append(s)\n",
    "\n",
    "    extended = [x for x in extended if x not in remove_words]\n",
    "    extended = [x for x in extended if len(x) >= 3]\n",
    "    extended = list(dict.fromkeys(extended))\n",
    "\n",
    "    useless_strings = list_from_txt('./data/bad_search_strings.txt')\n",
    "    for w in useless_strings:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    remove_words = ['Antibiotic U18496', 'D,L-Cyclophosphamide', 'N,O-propylen-phosphorescence-ester-diamid',\n",
    "                    'UNII-6UXW23996M component CMSMOCZEIVJLDB-CQSZACIVSA-N']\n",
    "    for w in remove_words:\n",
    "        try:\n",
    "            extended.remove(w)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return extended"
   ],
   "id": "705c53058a8103fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Aqui é o programa principal do _crawler_. Rodar isso aqui demora muito, então não sei ainda como vou testar.\n",
    "\n",
    "ALTERAÇÕES NECESSÁRIAS:\n",
    "\n",
    "   1. Termos de busca não são fixos, preciso de uma função auxiliar que retorna termos de busca para a doença de escolha.\n",
    "\n",
    "   2. Parecido com o 1, mas para os sinônimos.\n",
    "\n",
    "   3. Não acho que esse arquivo de IDs vai continuar existindo."
   ],
   "id": "a42bd370c491a485"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    destination_path = './results/'\n",
    "    Path('./data/ids.txt').touch(exist_ok=True)\n",
    "    Path(destination_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Cria uma lista com todos os termos de busca do arquivo. (não vai mais existir esse arquivo)\n",
    "    queries = list_from_txt('./data/queries.txt')\n",
    "    paper_counter = 0\n",
    "\n",
    "    # Insere os sinônimos dos compostos no fim da lista.\n",
    "    synonyms = extend_search()\n",
    "    queries.extend(synonyms)\n",
    "\n",
    "    # Cria uma lista com os IDs de todos os artigos já obtidos e um conjunto de IDs de artigos obtidos.\n",
    "    old_papers = list_from_txt('./data/ids.txt')\n",
    "    ids = set(old_papers)\n",
    "\n",
    "    # Para cada termo de busca...\n",
    "    for query in queries[100:]:\n",
    "        # Normaliza o termo de busca.\n",
    "        query = query.encode('ascii', 'ignore').decode('ascii')\n",
    "        print('searching for {}'.format(query))\n",
    "\n",
    "        # Procura pelo termo de busca, salva os IDs dos artigos encontrados em id_list.\n",
    "        id_list = list(search(query)['IdList'])\n",
    "        # Mantém só os que ainda não estão no conjunto de IDs conhecidos, ou seja, os novos.\n",
    "        id_list = [x for x in id_list if x not in ids]\n",
    "\n",
    "        # Se não achar nada novo, pode ir para o próximo termo de busca.\n",
    "        if not id_list:\n",
    "            print('No new papers found\\n')\n",
    "            continue\n",
    "\n",
    "        print('{} papers found\\n'.format(len(id_list)))\n",
    "        # Insere os novos IDs no conjunto.\n",
    "        ids.update(id_list)\n",
    "\n",
    "        # Pega os detalhes de cada artigo novo.\n",
    "        papers = fetch_details(id_list)\n",
    "\n",
    "        # Cria uma pasta com o nome do termo de busca, formatado para poder ser nome de pasta.\n",
    "        folder_name = query.lower().translate(str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "        Path(destination_path + '{}'.format(folder_name)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Para cada artigo novo...\n",
    "        for paper in papers['PubmedArticle']:\n",
    "            article_year = ''\n",
    "            article_abstract = ''\n",
    "\n",
    "            # Se não tiver título, vai para o próximo\n",
    "            try:\n",
    "                article_title = paper['MedlineCitation']['Article']['ArticleTitle']\n",
    "                article_title_filename = article_title.lower().translate(\n",
    "                    str.maketrans('', '', string.punctuation)).replace(' ', '_')\n",
    "            except KeyError as e: continue\n",
    "\n",
    "            # Se não tiver prefácio, vai para o próximo.\n",
    "            try:\n",
    "                article_abstract = ' '.join(paper['MedlineCitation']['Article']['Abstract']['AbstractText'])\n",
    "                article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']['Year']\n",
    "            except KeyError as e:\n",
    "                if 'Abstract' in e.args: continue\n",
    "                if 'Year' in e.args:\n",
    "                    article_year = paper['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate'][\n",
    "                                       'MedlineDate'][0:4]\n",
    "\n",
    "            # Se não tiver ano válido, vai para o próximo.\n",
    "            if len(article_year) != 4: continue\n",
    "\n",
    "            # O nome do arquivo vai ser \"{ano}_{nome_do_artigo}\"...\n",
    "            filename = '{}_{}'.format(article_year, article_title_filename)\n",
    "            if len(filename) > 150: filename = filename[0:146]\n",
    "\n",
    "            # e vai ser escrito naquela pasta criada antes do loop.\n",
    "            path_name = destination_path + folder_name + '/{}.txt'.format(filename)\n",
    "            path_name = path_name.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "            # Escreve o arquivo.\n",
    "            with open(path_name, \"a\", encoding='utf-8') as file:\n",
    "                file.write(article_title + ' ' + article_abstract)\n",
    "\n",
    "            paper_counter += 1\n",
    "\n",
    "        # Coloca os IDs dos artigos novos no arquivo.\n",
    "        with open('./data/ids.txt', 'a+', encoding='utf-8') as file:\n",
    "            for new_id in id_list: file.write('\\n' + str(new_id))\n",
    "\n",
    "    print('Crawler finished with {} papers collected.'.format(len(old_papers) + paper_counter))"
   ],
   "id": "a90551a1b1098c7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Agregação dos textos por ano\n",
    "\n",
    "Após pegar todos os prefácios resultantes das buscas, os textos precisam ser agrupados por ano para realizar aquela análise de quão antes o tratamento foi descoberto."
   ],
   "id": "5099efda3cae7e27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    start_year = 1900\n",
    "    end_year = 2025\n",
    "    source_path = './results/'\n",
    "    destination_path = './results_aggregated/'\n",
    "\n",
    "    # Pega os nomes de todos os arquivos que vieram do crawler.\n",
    "    filenames = list(map(str, Path(source_path).glob('**/*.txt')))\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "    # Para cada ano no intervalo...\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Pega os nomes dos arquivos de artigos que são do ano atual.\n",
    "        filenames_current_year = [f for f in filenames if Path(f).stem.startswith(str(year))]\n",
    "\n",
    "        # Se não tiver nenhum desse ano, vai para o próximo.\n",
    "        if not filenames_current_year:\n",
    "            continue\n",
    "\n",
    "        print(f'{len(filenames_current_year)} papers from {year}.\\n')\n",
    "\n",
    "        # Pega os prefácios de todos os artigos desse ano.\n",
    "        abstracts = []\n",
    "        for fname in filenames:\n",
    "            with open(fname, encoding='utf-8') as infile:\n",
    "                abstracts.extend(infile.readlines())\n",
    "\n",
    "        # Junta tudo em um só arquivo texto.\n",
    "        output_file = Path(destination_path) / f'results_file_1900_{year}.txt'\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for fname, abstract in zip(filenames, abstracts):\n",
    "                f.write(f\"{Path(fname).stem[5:]}|{abstract}\")"
   ],
   "id": "a678a4e378250ac9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Pré-processamento dos textos\n",
    "\n",
    "Esse é grande"
   ],
   "id": "421fac54a0c036aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "14fec3dc0ada7d1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
