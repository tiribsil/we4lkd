##################################################
## Generates the csv files containing the dot product between the embeddings for all compounds in the corpus and the target disease's.
##################################################

# IMPORTS:
import os, torch, shutil
import re
from pathlib import Path

from gensim.models import Word2Vec, FastText
import numpy as np
from nltk import word_tokenize
from scipy.special import softmax
import pandas as pd
from transformers import AutoTokenizer

from src.target_disease import *

os.chdir(Path(__file__).resolve().parent.parent)

def clear_hugging_face_cache_folder(dirpath='/home/ac4mvvb/.cache/huggingface/hub/'):
    """ Clears the Hugging Face cache folder, to prevent memory error.

    Args:
        dirpath: the path of the folder.
    """

    for filename in os.listdir(dirpath):
        filepath = os.path.join(dirpath, filename)
        try:
            shutil.rmtree(filepath)
        except OSError:
            os.remove(filepath)


def get_token_embedding(word, embedding_matrix, method='mean'):
    """Access and returns the token embedding from the model's embedding matrix for a given word.
    Depending on the method selected, it can return different composed token embeddings.

    Args:
        word: token that the user wants to tokenize and access the token embedding;
        embedding_matrix: the model's embedding matrix, generated by model.get_input_embeddings();
        method: the selected way to compose the token embedding
            mean: compute the mean between all the subwords generated by the tokenization of the input;
            first: select the token embedding of the first subword generated by the tokenization of the input;
            last: select the token embedding of the first subword generated by the tokenization of the input.

    Returns:
        A torch tensor of 768 dimensions representing the token embedding.
    """
    tokenizer = AutoTokenizer.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext")

    vocab_ids = []
    tokenized_word = tokenizer.tokenize(word)

    for w in tokenized_word:
        vocab_ids.append(tokenizer.vocab[w])

    word_token_embedding = None
    if method == 'mean':
        input_ids = torch.tensor(vocab_ids)
        word_token_embedding = embedding_matrix(input_ids)
        word_token_embedding = word_token_embedding.mean(0)

    elif method == 'first':
        input_ids = torch.tensor(vocab_ids[0])
        word_token_embedding = embedding_matrix(input_ids)

    elif method == 'last':
        input_ids = torch.tensor(vocab_ids[-1])
        word_token_embedding = embedding_matrix(input_ids)

    return word_token_embedding


def generate_compound_historical_record(compound, normalized_target_disease, c):
    for method in ['first', 'last', 'mean']:
        bert_embeddings_files = sorted(
            [f.path for f in os.scandir(f'./{normalized_target_disease}/validation/bert/') if f.name.endswith('.pt') and method in f.name])

        compound_dict = {
            'year': [],
            'dot_product_result': [],
            'dot_product_result_absolute': [],
            'softmax': [],
            'normalized_dot_product_absolute': [],
            'standardized_dot_product_absolute': [],
            'softmax_normalization': [],
            'softmax_standardization': [],
        }

        for be in bert_embeddings_files:
            tensor_dict = torch.load(be)

            try:
                index_compound = tensor_dict['compound'].index(c)
                index_AML = tensor_dict['compound'].index('AML')

                AML_we = tensor_dict['word_embedding'][index_AML]
                compound_we = tensor_dict['word_embedding'][index_compound]

            except:
                continue

            dot_product = torch.dot(compound_we, AML_we).type(torch.DoubleTensor).item()
            if dot_product > 0:
                dot_product_absolute = dot_product

            else:
                dot_product_absolute = -1 * dot_product

            compound_dict['year'].append(int(be.split('.pt')[0][-4:]))
            compound_dict['dot_product_result'].append(dot_product)
            compound_dict['dot_product_result_absolute'].append(dot_product_absolute)
        print('Dot products computed')

        # Softmax:
        compound_dict['softmax'] = softmax(compound_dict['dot_product_result_absolute'])

        # Normalization:
        maximum = np.max(compound_dict['dot_product_result_absolute'])
        compound_dict['normalized_dot_product_absolute'] = [x / maximum for x in
                                                            compound_dict['dot_product_result_absolute']]
        compound_dict['softmax_normalization'] = softmax(compound_dict['normalized_dot_product_absolute'])

        # Standardization:
        # compound_dict['softmax_standardization'] = preprocessing.scale(compound_dict['dot_product_result_absolute'])
        mean = np.mean(compound_dict['dot_product_result_absolute'])
        standard_deviation = np.std(compound_dict['dot_product_result_absolute'])
        compound_dict['standardized_dot_product_absolute'] = [(x - mean) / standard_deviation for x in
                                                              compound_dict['dot_product_result_absolute']]
        compound_dict['softmax_standardization'] = softmax(compound_dict['standardized_dot_product_absolute'])

        print("Writing compound's historical record .csv file, method {}".format(method))
        pd.DataFrame.from_dict(data=compound_dict).to_csv(
            './validation/per_compound/bert/{}_{}.csv'.format(compound, method),
            columns=['year', 'dot_product_result', 'dot_product_result_absolute', 'softmax',
                     'normalized_dot_product_absolute', 'standardized_dot_product_absolute', 'softmax_normalization',
                     'softmax_standardization'], index=False)


def get_w2v_output_embedding(word, model, method):
    """ Returns the output embedding of a given word from Word2Vec or FastText model.
        Updated for Gensim 4.0.0+.

    Args:
        word: the token in model's vocabulary that you want the output embedding;
        model: Word2Vec or FastText model object;
        method: how to access the output embedding:
            'da': direct access to the token (i.e., model.wv[word]) if it exists in the vocabulary.
                  If not, it searches for the first token in the vocabulary that contains 'word' as a substring.
            'avg': compute the average of all words in vocabulary that contains 'word' as a substring.
    """
    if method == 'da':
        # Tenta o acesso direto, que é a forma mais limpa e rápida.
        if word in model.wv.key_to_index:
            return model.wv[word]

        # Se falhar, busca pela primeira palavra que contém 'word' como substring.
        # Iteramos sobre a lista de chaves (palavras) do vocabulário.
        for key in model.wv.index_to_key:
            if word in key:
                # Retorna o vetor da primeira correspondência encontrada.
                return model.wv[key]

        # Se nenhuma correspondência for encontrada, retorna None ou um vetor de zeros.
        # Isso evita que o programa quebre se a palavra não for encontrada de forma alguma.
        # A escolha depende de como o código que chama esta função lida com falhas.
        # Retornar None é mais explícito.
        # Se precisar de um vetor, use np.zeros(model.vector_size)
        return None

    elif method == 'avg':
        # Encontra todas as palavras no vocabulário que contêm 'word' como substring.
        tokens_containing_the_word = [key for key in model.wv.index_to_key if word in key]

        if not tokens_containing_the_word:
            # Se a lista estiver vazia, nenhuma palavra contém a substring.
            # Retorna None ou um vetor de zeros.
            return None

        # Coleta os vetores de todas as palavras correspondentes.
        # O acesso model.wv[tokens_containing_the_word] já retorna uma matriz numpy dos vetores.
        # É muito mais eficiente do que um loop.
        output_embeddings = model.wv[tokens_containing_the_word]

        # Calcula a média ao longo do eixo 0 (a média dos vetores).
        return np.mean(output_embeddings, axis=0)

    else:
        # Lida com um métod inválido, se necessário.
        raise ValueError(f"Método '{method}' inválido. Escolha 'da' ou 'avg'.")


def get_compounds(normalized_target_disease):
    pubchem_path = 'data/pubchem_data/CID-Title'
    ner_table_path = f'./data/{normalized_target_disease}/corpus/ner_table.csv'

    pubchem_titles = pd.read_csv(
        pubchem_path,
        sep='\t',
        header=None,
        usecols=[1],
        names=['Title']
    )

    pubchem_normalized_set = set(
        pubchem_titles['Title'].str.lower().str.replace(r'\s+', '', regex=True).dropna()
    )

    ner_tokens = pd.read_csv(
        ner_table_path,
        usecols=['token']
    )

    ner_normalized_set = set(
        ner_tokens['token'].str.lower().str.replace(r'\s+', '', regex=True).dropna()
    )

    validated_normalized_compounds = list(pubchem_normalized_set & ner_normalized_set)

    print(f"Total de compostos validados: {len(validated_normalized_compounds)}")

    with open(f'./data/{normalized_target_disease}/corpus/compounds_in_corpus.txt', 'w', encoding='utf-8') as f:
        for compound in sorted(validated_normalized_compounds):
            f.write(f"{compound}\n")

    # Retorna a lista final de nomes já normalizados
    return validated_normalized_compounds

def main():
    target_disease = get_target_disease()
    normalized_target_disease = get_normalized_target_disease()

    print('Starting')

    # Loads all compounds present in the corpus.
    all_compounds_in_corpus = []
    if os.path.exists(f'./data/{normalized_target_disease}/corpus/compounds_in_corpus.txt'):
        with open(f'./data/{normalized_target_disease}/corpus/compounds_in_corpus.txt', 'w', encoding='utf-8') as f:
            all_compounds_in_corpus = [line.strip() for line in f if line.strip()]
    else:
        all_compounds_in_corpus = get_compounds()

    # Must be either 'w2v' or 'ft'.
    VALIDATION_TYPE = 'w2v'
    if VALIDATION_TYPE not in ['w2v', 'ft']:
        print('Invalid validation type, has to be either "w2v" or "ft".')
        exit(1)
    combination = '15' if VALIDATION_TYPE == 'w2v' else '16'

    model_directory_path = f'./data/{normalized_target_disease}/models/{VALIDATION_TYPE}_combination{combination}/'
    validation_directory_path = f'./data/{normalized_target_disease}/validation/{VALIDATION_TYPE}/compound_history/'

    os.makedirs(model_directory_path, exist_ok=True)
    os.makedirs(validation_directory_path, exist_ok=True)

    # Loads each of the year range trained models.
    models = sorted([f.path for f in os.scandir(model_directory_path) if f.name.endswith('.model')])
    
    # Initializes a dictionary to store various metrics for each compound.
    # Each metric is a way to measure the relationship between the compound and the target disease.
    dictionary_for_all_compounds = {}
    for c in all_compounds_in_corpus:
        dictionary_for_all_compounds.update({
            f'{c}_comb{combination}': {
                'year': [],
                'dot_product_result': [],
                'dot_product_result_absolute': [],
                'softmax': [],
                'normalized_dot_product_absolute': [],
                'standardized_dot_product_absolute': [],
                'softmax_normalization': [],
                'softmax_standardization': [],
            }
        }
        )

    # List of years until which the models were trained.
    years = []

    for file in os.listdir(f'./data/{normalized_target_disease}/corpus/aggregated_abstracts/'):
        file_path = f'./data/{normalized_target_disease}/corpus/aggregated_abstracts/{file}'

        filename, extension = os.path.splitext(file_path)
        years.append(int(filename[-4:]))

    years = sorted(years)

    print('VALIDATION_TYPE: ', VALIDATION_TYPE)
    print(f'Years: {years[0]} to {years[1]}')
    
    # For each year until which a model was trained...
    for y in years:
        print(f'\nCurrent year of analysis: {y}')

        if VALIDATION_TYPE == 'w2v':
            if not models:
                print('No Word2Vec models found in the directory.')
                continue

            # Loads the Word2Vec model that was trained until year y.
            print('Loading Word2Vec model')
            model = Word2Vec.load([x for x in models if str(y) in x][0])

            # Accesses the word embedding of the target disease.
            try:
                target_disease_we = model.wv[normalized_target_disease]
                print(f"Accessing the word embedding of '{normalized_target_disease}'")
            except:
                print('Target disease is not in the vocabulary')
                continue

            # Tries to access the embedding of each of the compounds.
            for compound in all_compounds_in_corpus:
                print(f'Accessing the output embedding of {compound}')
                compound_we = get_w2v_output_embedding(compound, model, method='da')
                if compound_we is None:
                    print(f"Compound '{compound}' not found in the model's vocabulary.")
                    continue

                # Computes the dot product between the compound's embedding and the target disease's embedding.
                dot_product = np.dot(compound_we, target_disease_we).item()

                # Fills out the corresponding dictionary entry with the results.
                dictionary_for_all_compounds[f'{compound}_comb15']['year'].append(y)
                dictionary_for_all_compounds[f'{compound}_comb15']['dot_product_result'].append(dot_product)
                dictionary_for_all_compounds[f'{compound}_comb15']['dot_product_result_absolute'].append(abs(dot_product))

        elif VALIDATION_TYPE == 'ft':
            if not models:
                print('No FastText models found in the directory.')
                continue
                
            # Loads the FastText model that was trained until year y.
            print('Loading FastText model')
            model_comb16 = FastText.load([x for x in models if str(y) in x][0])

            # Accesses the word embedding of the target disease.
            try:
                target_disease_we = model_comb16.wv[normalized_target_disease]
                print(f"Accessing the word embedding of '{normalized_target_disease}'")
            except:
                print('Target disease is not in the vocabulary')
                continue

            # Tries to access the embedding of each of the compounds.
            for compound in all_compounds_in_corpus:
                print('Accessing the output embedding of {}'.format(compound))
                compound_we = get_w2v_output_embedding(compound, model_comb16, method='da')
                if compound_we is None:
                    print(f"Compound '{compound}' not found in the model's vocabulary.")
                    continue

                # Computes the dot product between the compound's embedding and the target disease's embedding.
                dot_product = np.dot(compound_we, target_disease_we).item()

                # Fills out the corresponding dictionary entry with the results.
                dictionary_for_all_compounds[f'{compound}_comb16']['year'].append(y)
                dictionary_for_all_compounds[f'{compound}_comb16']['dot_product_result'].append(dot_product)
                dictionary_for_all_compounds[f'{compound}_comb16']['dot_product_result_absolute'].append(abs(dot_product))

    # For each compound...
    print('Generating historical record for each compound')
    for c in all_compounds_in_corpus:
        key = f'{c}_comb{combination}'
        key_filename = re.sub(r'[\\/*?:"<>|]', '_', key)

        # Gets the absolute dot products for the current compound. Will be used for further calculations.
        dot_products_abs = dictionary_for_all_compounds[key]['dot_product_result_absolute']

        # Skips the compound if it was never iterated upon (i.e., no dot products were computed).
        # This SHOULD NOT happen, as the compounds were validated against the corpus. TODO
        if not dot_products_abs:
            print(f"Aviso: Lista de dados para '{key}' está vazia. Pulando cálculos.")
            continue

        # Computes the softmax of the absolute dot products and stores it in the dictionary.
        dictionary_for_all_compounds[key]['softmax'] = softmax(dot_products_abs)

        # Gets the normalized dot products and softmaxes and stores them in the dictionary.
        maximum = np.max(dot_products_abs)
        if maximum > 0:
            normalized_values = [x / maximum for x in dot_products_abs]
            dictionary_for_all_compounds[key]['normalized_dot_product_absolute'] = normalized_values
            dictionary_for_all_compounds[key]['softmax_normalization'] = softmax(normalized_values)
        else:
            # Should not happen, but this prevents division by zero.
            dictionary_for_all_compounds[key]['normalized_dot_product_absolute'] = [0.0] * len(dot_products_abs)
            dictionary_for_all_compounds[key]['softmax_normalization'] = softmax([0.0] * len(dot_products_abs))

        # Standard deviation requires at least two data points to not result in a division by 0.
        if len(dot_products_abs) > 1:
            mean = np.mean(dot_products_abs)
            standard_deviation = np.std(dot_products_abs)

            if standard_deviation > 0:
                # Computes the standardized dot products and softmaxes and stores them in the dictionary.
                standardized_values = [(x - mean) / standard_deviation for x in dot_products_abs]
                dictionary_for_all_compounds[key]['standardized_dot_product_absolute'] = standardized_values
                dictionary_for_all_compounds[key]['softmax_standardization'] = softmax(standardized_values)
            else:
                # If the standard deviation is 0, all values are the same.
                dictionary_for_all_compounds[key]['standardized_dot_product_absolute'] = [0.0] * len(dot_products_abs)
                dictionary_for_all_compounds[key]['softmax_standardization'] = softmax([0.0] * len(dot_products_abs))
        else:
            # If there's only one data point, standard deviation is 0.
            dictionary_for_all_compounds[key]['standardized_dot_product_absolute'] = [0.0]
            dictionary_for_all_compounds[key]['softmax_standardization'] = softmax([0.0])

        # Writes the compound entry in the dictionary to a CSV file.
        print(f'Writing file for {key}')
        pd.DataFrame.from_dict(data=dictionary_for_all_compounds[key]).to_csv(
            f'{validation_directory_path}/{key_filename}.csv',
            columns=['year', 'dot_product_result', 'dot_product_result_absolute', 'softmax',
                     'normalized_dot_product_absolute', 'standardized_dot_product_absolute',
                     'softmax_normalization', 'softmax_standardization'],
            index=False
        )

    print('End :)')

if __name__ == '__main__':
    main()