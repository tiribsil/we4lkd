import os
import sys
from datetime import datetime
from pathlib import Path

# Ensure the 'src' directory is in the Python path
os.chdir(Path(__file__).resolve().parent)
sys.path.append(str(Path.cwd()))

from src.utils import get_target_disease, get_normalized_target_disease, set_target_disease
from src.crawler import run_pubmed_crawler
from src.merge_txt import aggregate_abstracts_from_year
from src.ner_table_generator import generate_ner_table
from src.clean_summaries import clean_and_normalize_abstracts
from src.train_yoy import train_word_embedding_models
from src.generate_dotproducts_csv import generate_compound_dot_products_csv
from src.latent_knowledge_report import generate_latent_knowledge_report
from src.get_best_treatment_candidates import main as get_best_treatment_candidates


def feedback_new_topics(normalized_target_disease: str):
    """
    Reads potential treatments and adds them to the topics of interest file.

    This function reads the list of potential treatments generated by the
    'get_best_treatment_candidates' script and merges them with the existing
    'topics_of_interest.txt' file. It uses sets to ensure that no duplicate
    topics are added.

    Args:
        normalized_target_disease (str): The normalized name of the target
                                         disease, used for constructing file paths.
    """
    potential_treatments_file = Path(f'./data/{normalized_target_disease}/potential_treatments.txt')
    topics_file = Path(f'./data/{normalized_target_disease}/topics_of_interest.txt')

    if not potential_treatments_file.exists():
        print("Warning: 'potential_treatments.txt' not found. Skipping feedback loop.")
        return

    try:
        # Read current topics, handling the case where the file might not exist yet.
        if topics_file.exists():
            with open(topics_file, 'r', encoding='utf-8') as f:
                existing_topics = set(line.strip() for line in f if line.strip())
        else:
            existing_topics = set()

        # Read new potential treatments.
        with open(potential_treatments_file, 'r', encoding='utf-8') as f:
            new_treatments = set(line.strip() for line in f if line.strip())

        # Unite the two sets to automatically handle duplicates.
        combined_topics = existing_topics.union(new_treatments)

        # Write the updated and sorted list back to the topics file.
        with open(topics_file, 'w', encoding='utf-8') as f:
            for topic in sorted(list(combined_topics)):
                f.write(f"{topic}\n")

        print(f"Feedback complete: {len(new_treatments)} new potential treatments considered.")
        print(f"Total topics of interest are now {len(combined_topics)}.")

    except IOError as e:
        print(f"Error during feedback loop: {e}")


def main_pipeline():
    """
    Main function to run the entire pipeline from data crawling to report generation.
    """
    # --- Configuration ---
    # The disease name is now expected to be in 'target_disease.txt'.
    target_disease = get_target_disease()
    if not target_disease:
        print("Error: Target disease not set. Please create a 'target_disease.txt' file with the disease name.")
        return

    normalized_target_disease = get_normalized_target_disease()
    print(f"--- Starting Pipeline for: {target_disease} ---")

    # The start year can be configured here.
    start_year = 1970
    end_year = datetime.now().year

    # --- Pipeline Execution Loop ---
    for current_year in range(start_year, end_year + 1):
        print(f"\n{'='*20} Processing year: {current_year} {'='*20}")

        # Step 1: Crawl PubMed for abstracts up to the current year.
        print("\n--- Step 1: Running PubMed Crawler ---")
        # We pass the date range to the crawler function.
        run_pubmed_crawler(
            target_disease=target_disease,
            normalized_target_disease=normalized_target_disease,
            start_year=start_year,
            end_year=current_year
        )

        # Step 2: Aggregate abstracts into files from year.
        print("\n--- Step 2: Aggregating Abstracts ---")
        aggregate_abstracts_from_year(normalized_target_disease, current_year)

        # Step 3: Generate NER table from the latest aggregated file.
        print("\n--- Step 3: Generating NER Table ---")
        generate_ner_table(target_disease, normalized_target_disease)

        # Step 4: Clean and normalize the abstracts.
        print("\n--- Step 4: Cleaning and Normalizing Abstracts ---")
        clean_and_normalize_abstracts(target_disease, normalized_target_disease)

        # Step 5: Train Word2Vec/FastText models year-over-year.
        print("\n--- Step 5: Training Word Embedding Models ---")
        train_word_embedding_models(normalized_target_disease, start_year, current_year)

        # Step 6: Generate dot products CSV for compound-disease relationships.
        print("\n--- Step 6: Generating Compound Dot Products ---")
        generate_compound_dot_products_csv(normalized_target_disease)

        # Step 7: Generate the latent knowledge report.
        print("\n--- Step 7: Generating Latent Knowledge Report ---")
        generate_latent_knowledge_report(target_disease, normalized_target_disease)

        # Step 8: Get the best treatment candidates based on the analysis.
        print("\n--- Step 8: Identifying Best Treatment Candidates ---")
        get_best_treatment_candidates()

        # Step 9: Backfeed the results into the topics of interest for the next iteration.
        print("\n--- Step 9: Feedback Loop for New Topics ---")
        feedback_new_topics(normalized_target_disease)

    print(f"\n--- Pipeline finished successfully for {target_disease} up to year {end_year} ---")


if __name__ == '__main__':
    set_target_disease("acute myeloid leukemia")
    main_pipeline()
